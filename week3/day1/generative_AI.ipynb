{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "169da616",
   "metadata": {},
   "source": [
    "## U-Net\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*qNdglJ1ORP3Gq77MmBLhHQ.png\">\n",
    "\n",
    "U-Net은 기본적인 인코더-디코더 모델이며, **Convolution block**과 **skip connection**을 사용하여 **image segmentation**에 특화된 모델이다.\n",
    "\n",
    "- 인코더 \n",
    "    - 일반적인 컨볼루션 신경망(CNN)처럼 2개의 컨볼루션 블록과 1개의 max pooling으로 구성된 단계를 반복\n",
    "    - Convolution block은 보통 convolution, activation function, normalization으로 이루어져 있음\n",
    "    - 이미지의 공간적(local feature) 해상도를 줄이면서 (Down sampling) 채널 수를 늘려 특징을 추출하고, 전반적인 맥락 정보 (context information)를 학습\n",
    "\n",
    "- 디코더\n",
    "    - 인코더와 대칭적인 구조로, 전치 컨볼루션(Transpose Convolution) 또는 업샘플링(Upsampling)을 통해 특징 맵의 크기를 늘림\n",
    "    - 이 과정에서 채널 수를 줄이고 공간적 해상도를 복원\n",
    "    - 최종적으로 입력 이미지와 동일한 크기의 출력 맵을 만듦 \n",
    "\n",
    "- 스킵 커넥션\n",
    "    - U-Net의 인코더의 각 단계에서 특징 맵을 추출하고 이 맵을 디코더의 대칭되는 단계로 직접 연결(concatenate)함\n",
    "    - 인코더의 다운 샘플링 과정에서 손실될 수 있는 세부적인 지역 공간 정보를 디코더에 전달하여, 더 정확한 경계와 위치를 예측할 수 있도록 함\n",
    "\n",
    "---\n",
    "\n",
    "### Convoulution 에서의 Stride와 Padding\n",
    "\n",
    "- Stride: 필터(kernel)가 입력 이미지 위를 이동하는 **간격**을 의미한다. 스트라이드가 2이면 필터가 두칸씩 이동하며, 이로 인해 출력 특징 맵의 크기는 줄어든다. 즉 image의 특징을 요약하는 것과 유사하다. \n",
    "\n",
    "- Padding: 입력 이미지의 가장자리의 0을 추가하여 필터가 가장자리 픽셀에 충분히 접근할 수 있도록 돕는다. 이를 통해 출력 특징 맵의 크기가 입력과 동일하거나 비슷하게 유지될 수 있다.\n",
    "\n",
    "### Transpose Convolution 에서의 Stride와 Padding \n",
    "\n",
    "- Stride: 전치 컨볼루션에서 스트라이드는 입력 픽셀 사이에 삽입되는 0의 개수를 결정하는 역할을 한다. 스트라이드가 2라면, 입력 특징 맵의 각 픽셀 사이에 1줄(stride - 1)의 0이 삽입 되어 이미지의 크기를 늘린다. 따라서 Transpose Convolution에서의 stride는 이미지의 크기를 확장하는 업샘플링, 즉 디코더에서 사용된다.\n",
    "\n",
    "- Padding: 출력 이미지의 경계에서 얼마나 많은 픽셀을 제거할지를 결정한다. 즉, 입력에 0을 채우는 것이 아니라 계산된 출력에서 일부분을 잘라내는 역할을 한다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529fb71",
   "metadata": {},
   "source": [
    "### U-Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3224a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 16 # Due to stride and pooling, must be divisible by 2 multiple times\n",
    "IMG_CH = 1 # Black and white image, no color channels\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d15bc4",
   "metadata": {},
   "source": [
    "### Down Block\n",
    "\n",
    "Convolution layers를 통해 입력 이미지에서 지역 정보를 추출하는 다운 샘플링 방식이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff098fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        padding = 1\n",
    "        \n",
    "        super().__init__()\n",
    "        # Convolution Block \n",
    "        layers = [\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding),\n",
    "            nn.BatchNorm1d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size, stride, padding),\n",
    "            nn.BatchNorm1d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        ]\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea0c4d",
   "metadata": {},
   "source": [
    "### UpBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0cc55f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        # Convolution variables\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        padding = 1\n",
    "\n",
    "        # Transpose variables\n",
    "        strideT = 2 # 값과 값 사이에 0의 값 추가 \n",
    "        out_paddingT = 1\n",
    "\n",
    "        super().__init__()\n",
    "        # 2 * in_chs for concatednated skip connection\n",
    "        # Convolution Block의 구성을 유사하지만 stride와 padding의 역할이 다름 \n",
    "        layers = [\n",
    "            # Down의 마지막 feature map이 그대로 Up의 처음 feature map에 더해짐 -> 섬세한 복원 가능 \n",
    "            # 따라서 in_channel의 값이 2배임 \n",
    "            nn.ConvTranspose2d(2 * in_ch, out_ch, kernel_size, strideT, padding, out_paddingT),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    # skip: encoder의 대칭되는 위치에서 넘어온 tensor \n",
    "    # torch.cat((x, skip))은 x와 skip 텐서를 채널을 기준으로 concate\n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat((x, skip), dim = 1)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613c2b4b",
   "metadata": {},
   "source": [
    "### Full U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b5a0666",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        img_ch = IMG_CH\n",
    "        down_chs = (16, 32, 64)\n",
    "        up_chs = down_chs[::-1]  # Reverse of the down channels\n",
    "        latent_image_size = IMG_SIZE // 4 # 2 ** (len(down_chs) - 1)\n",
    "\n",
    "        # Inital convolution\n",
    "        self.down0 = nn.Sequential(\n",
    "            nn.Conv2d(img_ch, down_chs[0], 3, padding=1),\n",
    "            nn.BatchNorm2d(down_chs[0]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Downsample\n",
    "        self.down1 = DownBlock(down_chs[0], down_chs[1])\n",
    "        self.down2 = DownBlock(down_chs[1], down_chs[2])\n",
    "        self.to_vec = nn.Sequential(nn.Flatten(), nn.ReLU())\n",
    "        \n",
    "        # Embeddings\n",
    "        self.dense_emb = nn.Sequential(\n",
    "            nn.Linear(down_chs[2]*latent_image_size**2, down_chs[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(down_chs[1], down_chs[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(down_chs[1], down_chs[2]*latent_image_size**2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Upsample\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.Unflatten(1, (up_chs[0], latent_image_size, latent_image_size)),\n",
    "            nn.Conv2d(up_chs[0], up_chs[0], 3, padding=1),\n",
    "            nn.BatchNorm2d(up_chs[0]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UpBlock(up_chs[0], up_chs[1])\n",
    "        self.up2 = UpBlock(up_chs[1], up_chs[2])\n",
    "\n",
    "        # Match output channels\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(up_chs[-1], up_chs[-1], 3, 1, 1),\n",
    "            nn.BatchNorm2d(up_chs[-1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(up_chs[-1], img_ch, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        down0 = self.down0(x)\n",
    "        down1 = self.down1(down0)\n",
    "        down2 = self.down2(down1)\n",
    "        latent_vec = self.to_vec(down2)\n",
    "\n",
    "        up0 = self.up0(latent_vec)\n",
    "        up1 = self.up1(up0, down2)\n",
    "        up2 = self.up2(up1, down1)\n",
    "        return self.out(up2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "901bc611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params:  234977\n"
     ]
    }
   ],
   "source": [
    "model = UNet()\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3339be92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
