{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54eafbb0",
   "metadata": {},
   "source": [
    "### LangChain's `PyPDFLoader`, `PyMuPDFLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "12f037dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader # 솓도 최적화가 되어 있는 로더 \n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url = \"http://localhost:11434\",\n",
    "    model = \"gemma3:latest\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "llm.model\n",
    "\n",
    "FILE_PATH = \"./data/Context-Aware Named Entity Recognition for Neologism Detection- A Deep Learning Approach to Bridging Generational Language Gaps.pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2f5320a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 438 ms\n",
      "Wall time: 455 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "loader = PyPDFLoader(FILE_PATH)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "36cd4965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-Aware Named Entity Recognition \n",
      "for Neologism Detection: A Deep Learning \n",
      "Approach to Bridging Generational \n",
      "Language Gaps  \n",
      "Junhaeng Lee,  HyunYoung Oh , SangWoo Park, and Young -myoung Kang*  \n",
      "Abstract—The rapid proliferation of neologisms in online and social communication has intensified linguistic \n",
      "disparities across generations, posing challenges for timely and accurate neologism detection. Conventional \n",
      "dictionary- and rule-based methods struggle to ca pture the dynamic, context -dependent nature of evolving \n",
      "expressions, limiting their practical applicability. This study proposes a deep learning -based, context-aware \n",
      "Named Entity Recognition (NER) model designed to effectively identify neologisms and mitigate generational \n",
      "language gaps. The model leverages KoELECTRA, a pre -trained Korean language model optimized for \n",
      "contextual understanding,  and is evaluated on large -scale datasets with varying entity distributions. \n",
      "Experimental results demonstrate that the proposed model achieves robust neologism detection \n",
      "performance, with an F1 -score of 0.89 under real -world imbalanced conditions and up to 0.92 in controlled, \n",
      "uniform settings. Furthermore, the model exhibits balanced recognition across diverse e ntity types, \n",
      "confirming its applicability to both formal and informal language environments. These findings highlight the \n",
      "model's potential as a scalable solution for enhancing linguistic accessibility and supporting real -time \n",
      "adaptation to evolving language, particularly in bridging generational communication gaps. \n",
      "Research Keywords—Deep Learning, Generational Communication, Korean Neologisms, Named Entity \n",
      "Recognition, Natural Language Processing. \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――― \n",
      "1 INTRODUCTION\n",
      "In contemporary society, conflicts driven by na-\n",
      "tional, racial, religious, and generational divides are \n",
      "escalating globally. Among these, generational con-\n",
      "flict has become a critical social concern in South \n",
      "Korea. According to a 2025 national survey, nearly \n",
      "80% of respondents identified generational conflict \n",
      "as a serious issue, primarily attributing it to com-\n",
      "munication barriers and collaboration difficulties \n",
      "across age groups [1]. Notably, these challenges ex-\n",
      "tend beyond differences in value systems, originat-\n",
      "ing from fundamental discrepancies in linguistic ex-\n",
      "pression patterns and communication modalities \n",
      "between generations. \n",
      "The proliferation of internet -based platforms \n",
      "and social media has significantly accelerated the \n",
      "emergence and diffusion of neologisms, which now \n",
      "serve as integral components of everyday commu-\n",
      "nication, transcending conventional generational \n",
      "boundaries [2]. In the South Korean context, neolo-\n",
      "gisms are extensively utilized across mass media, \n",
      "broadcasting, and public discourse, functioning as \n",
      "key linguistic elements that shape socio-cultural in-\n",
      "teraction [3]. However, intergenerational dispari-\n",
      "ties in the recogni tion, comprehension, and utiliza-\n",
      "tion of neologisms persist, exacerbating inequalities \n",
      "not only in the transmission but also in the interpre-\n",
      "tation and accessibility of information [4].  Conven-\n",
      "tional approaches to neologism detection primarily \n",
      "rely on static dictionary -based methodologies, \n",
      "———————————————— \n",
      "• Junhaeng Lee is with the Department of Computer Engineer-\n",
      "ing, Sungkyul University, Anyang, Korea. E-mail: \n",
      "hkhk9495@sunkyul.ac.kr. \n",
      "• HyunYoung Oh is with the Department of Computer Engineer-\n",
      "ing, Sungkyul University, Anyang, Korea. E-mail: \n",
      "ohy414@sungkyul.ac.kr. \n",
      "• SangWoo Park is with the Department of Computer Engineer-\n",
      "ing, Sungkyul University, Anyang, Korea. E-mail: \n",
      "sgpswoo@sungkyul.ac.kr. \n",
      "• Young-myoung Kang (corresponding author) is with the De-\n",
      "partment of Computer Engineering, Sungkyul University, An-\n",
      "yang, Korea. E-mail: ykang@sungkyul.ac.kr.\n",
      "where unregistered word forms are identified by \n",
      "comparing word frequencies within a target corpus \n",
      "against pre-defined lexicons [5]. While such meth-\n",
      "ods are effective in detecting neologisms with high \n",
      "linguistic stability, they exhibit significant limita-\n",
      "tions in capturing transient, context -sensitive, and \n",
      "rapidly evolving expressions characteristic of dy-\n",
      "namic online language environments. Moreover, \n",
      "these approaches lack adaptability to account for \n",
      "the real-time linguistic innovation cycles prevalent \n",
      "in digital communication spaces. \n",
      "This study defines two core limitations underly-\n",
      "ing existing methodologies: (1) the inability to ac-\n",
      "commodate the temporal volatility and context de-\n",
      "pendency inherent in neologism generation and \n",
      "dissemination; and (2) the amplification of inter-\n",
      "generational linguistic asymmetries, which impairs \n",
      "effective communication and perpetuates dispari-\n",
      "ties in information accessibility. To overcome these \n",
      "limitations, we propose a deep learning -based \n",
      "Named Entity Recognition (NER) framework opti-\n",
      "mized for the detection of ne ologisms within con-\n",
      "text-sensitive environments. The proposed model is \n",
      "trained on a large-scale Korean web corpus [6], pre-\n",
      "dominantly composed of news media content, and \n",
      "annotated with seven entity categories: Organiza-\n",
      "tion (OG), Artifact (AF), Civilization (CV), Field of \n",
      "Study (FD), Theory/Rule (TR), Term (TM), and Ne-\n",
      "ologism (NW). The integration of neologism detec-\n",
      "tion w ithin a multi -entity recognition architecture \n",
      "enhances the model’s generalizability and practical \n",
      "applicability across diverse linguistic set tings, con-\n",
      "tributing to the mitigation of generational commu-\n",
      "nication gaps. \n",
      "To evaluate the robustness and performance \n",
      "consistency of the proposed model under varying \n",
      "data distributions, three dataset variants were con-\n",
      "structed: Original, Balanced, and Uniform. The \n",
      "model architecture employs KoELECTRA [ 7], a Ko-\n",
      "rean language-adapted version of the Transformer-\n",
      "based ELECTRA framework [8], pre-trained primar-\n",
      "ily on news corpora to optimize the recognition of \n",
      "structured linguistic patterns and context -depend-\n",
      "ent expressions. Experimental results indicate that \n",
      "the proposed model achieves a macro-averaged F1-\n",
      "score of 0.88 and a neologism -specific F1 -score of \n",
      "0.91 under the Balanced dataset configuration, \n",
      "demonstrating superior performance in detecting \n",
      "evolving and contextually grounded linguistic \n",
      "forms. These findings validate the model’s ca pacity \n",
      "to address the limitations of conventional diction-\n",
      "ary- or rule-based approaches and highlight its po-\n",
      "tential to facilitate intergenerational linguistic inte-\n",
      "gration, thereby promoting equitable information \n",
      "access and enhancing efficiency in social com muni-\n",
      "cation. \n",
      "2 RELATED WORK \n",
      "2.1 Neologism Collection \n",
      "Traditional approaches to neologism collection \n",
      "have primarily relied on extracting candidate terms \n",
      "from various online platforms, including internet \n",
      "communities, blogs, social media, and collaborative \n",
      "knowledge bases such as Wikipedia [4]. In this pro-\n",
      "cess, words that exhibited low frequency within the \n",
      "collected corpus and were absent from existing dic-\n",
      "tionaries were flagged as potential neologisms. \n",
      "These candidates were then compared against cu-\n",
      "rated lexicons comprising well -defined terms to fi-\n",
      "nalize the neol ogism list [ 9]. Through iterative re-\n",
      "finement, existing dictionaries were either ex-\n",
      "panded or updated to incorporate newly identified \n",
      "expressions. \n",
      "However, neologisms in contemporary digital \n",
      "environments exhibit inherently unstable life cycles, \n",
      "characterized by rapid emergence, dissemination, \n",
      "and disappearance. Their meanings and usage pat-\n",
      "terns are highly fluid, often varying across \n",
      "timeframes, onlin e subcultures, and communities. \n",
      "Consequently, static lexicon- or rule-based methods \n",
      "struggle to effectively capture this linguistic volatil-\n",
      "ity. Furthermore, neologisms frequently manifest in \n",
      "diverse forms—including low-frequency tokens, in-\n",
      "tentional misspellings, abbreviations, hybrid struc-\n",
      "tures, and combinations with emojis—complicating \n",
      "their detection through standardized pattern recog-\n",
      "nition techniques. The semantic ambiguity or poly-\n",
      "semy associated with many neologisms presents \n",
      "additional challenges for deep learning models, par-\n",
      "ticularly concerning the acquisition of sufficient, \n",
      "high-quality annotated data required for accurate \n",
      "embedding learning [9]. \n",
      "2.2 Named Entity Recognition \n",
      "Named Entity Recognition (NER) is a fundamental \n",
      "subtask in natural language processing (NLP) that \n",
      "involves identifying and classifying entities within \n",
      "unstructured text, including persons, locations, or-\n",
      "ganizations, temporal expressions, and  domain-\n",
      "specific entities such as chemical compounds or \n",
      "pharmaceutical products [10]. \n",
      "Initial research in NER predominantly utilized \n",
      "rule-based approaches, leveraging handcrafted se-\n",
      "mantic, syntactic, and pattern -based rules in con-\n",
      "junction with domain -specific dictionaries. While \n",
      "these methods achieved high precision within nar-\n",
      "row domains, they suffered from limited scalability\n",
      "and low recall, primarily due to the incompleteness \n",
      "of lexicons and the rigidity of manually crafted rules \n",
      "[11]. \n",
      "The advent of deep learning has significantly ad-\n",
      "vanced NER methodologies, with neural network -\n",
      "based models demonstrating superior performance \n",
      "over traditional rule -based systems [1 1]. In partic-\n",
      "ular, the Bidirectional Long Short -Term Memory \n",
      "with Conditional Random Fields (BiLSTM -CRF) ar-\n",
      "chitecture has become a widely adopted framework, \n",
      "effectively capturing bidirectional contextual de-\n",
      "pendencies within sentences and improving se-\n",
      "quential labeling accuracy [12]. \n",
      "More recently, the introduction of pre -trained \n",
      "language models and Transformer -based architec-\n",
      "tures has revolutionized NER by enabling context -\n",
      "aware representation learning without the need for \n",
      "manual feature engineering. These models, trained \n",
      "on large-scale corpora, acquire generalized linguis-\n",
      "tic knowledge that facilitates high -performance \n",
      "NER, even in scenarios with limited labeled data \n",
      "[13]. Their ability to effectively capture complex se-\n",
      "mantic relationships has significantly improved \n",
      "NER accuracy across diverse languages and do-\n",
      "mains. \n",
      "3 METHODOLOGY \n",
      "3.1 Data set \n",
      "The dataset utilized in this study is a large -scale, \n",
      "web-based Korean corpus comprising \n",
      "approximately one billion words, released in 2021. \n",
      "The corpus primarily consists of news articles \n",
      "spanning 17 thematic categories, including society, \n",
      "culture, science, and economics. To facilitate entity \n",
      "recognition tasks, the dataset was annotated with \n",
      "seven entity types: Organization (OG), Artifact (AF), \n",
      "Civilization (CV), Field of Study (FD), Theory/Rule \n",
      "(TR), Term (TM), and Neologism (NW). The \n",
      "annotation process was conducted using an \n",
      "automated authoring tool to ensure large -scale \n",
      "consistency. For neologism tagging, only newly \n",
      "emerging terms that appeared with a certain \n",
      "frequency but were absent from standardized \n",
      "Korean dictionaries were labeled accordingly. \n",
      "Given the inherent ambiguity and inconsistency \n",
      "in the definition of neologisms in prior works, this \n",
      "study established explicit classification criteria to \n",
      "ensure consistency and clarity in neologism \n",
      "identification. Specifically, neologisms were strictly \n",
      "limited to the following categories: \n",
      "• Compound expressions  formed by the combi-\n",
      "nation of two or more existing words, \n",
      "• Hybrid terms  resulting from partial word \n",
      "combinations or morphological blending, \n",
      "• Loanwords or semantic extensions of pre -ex-\n",
      "isting terms introduced into Korean. \n",
      "Any expressions falling outside these categories \n",
      "were excluded from the neologism label set [14]. To \n",
      "maintain alignment between the refined neologism \n",
      "criteria and the overall dataset structure, a \n",
      "proportional downsampling strategy was applied, \n",
      "ensuring that the entity distributions within the \n",
      "dataset reflected the updated classification \n",
      "boundaries. \n",
      "An analysis of the dataset revealed significant \n",
      "distributional imbalance across entity types. In par-\n",
      "ticular, Organization (OG) and Civilization (CV) en-\n",
      "tities dominated the corpus, which is attributable to \n",
      "the corpus's content composition, heavily focused \n",
      "on socio -cultural narratives, government entities, \n",
      "corporate organizations, and popular culture. Con-\n",
      "versely, entities associated with specialized or do-\n",
      "main-specific expressions, such as Neologism (NW) \n",
      "and Theory/Rule (TR), were comparatively scarce, \n",
      "reflecting their lower natural occurrence within \n",
      "mainstream media texts.  Table 1 presents a sum-\n",
      "mary of the entity type distribution, illustrating \n",
      "both the original and post -filtering instance counts \n",
      "for each category. \n",
      "These distributional patterns highlight the chal-\n",
      "lenges associated with low -resource entity types \n",
      "such as neologisms, underscoring the importance of \n",
      "developing robust models capable of accurately de-\n",
      "tecting rare and context -sensitive expressions \n",
      "within imbalanced datasets. \n",
      "3.2 Model \n",
      "The widely used BERT model [1 5] conducts pre -\n",
      "training via Masked Language Modeling (MLM) \n",
      "which suffers from suboptimal data utilization and \n",
      "distributional mismatch between pre -training and \n",
      "fine-tuning [8]. \n",
      "To address these limitations, the ELECTRA \n",
      "model introduces a more efficient pre -training par-\n",
      "adigm known as Replaced Token Detection (RTD). \n",
      "Table 1. Named Entities Before and After Filtering\n",
      "ELECTRA employs a dual-Transformer architecture \n",
      "consisting of a Generator and a Discriminator. The \n",
      "Generator produces plausible replacements for ran-\n",
      "domly selected tokens, while the Discriminator per-\n",
      "forms token -level binary classification to distin-\n",
      "guish between original and replaced tokens. Unlike \n",
      "MLM, this approach eliminates the need for artifi-\n",
      "cial mask tokens, preserving the natural contextual \n",
      "flow of input sequences while enabling loss compu-\n",
      "tation across all tokens. As a result, ELECTRA \n",
      "achieves superior learning efficiency and improved \n",
      "alignment between pre -training and downstream \n",
      "tasks. The overall model architecture and training \n",
      "mechanism are depicted in Fig. 1. \n",
      "In this study, we adopted KoELECTRA, a Korean \n",
      "language-optimized variant of ELECTRA. KoELEC-\n",
      "TRA was pre-trained on a large-scale Korean corpus \n",
      "comprising approximately 14 GB of text, including \n",
      "diverse sources such as news articles, encyclopedic \n",
      "entries, and Namu Wiki content. This corpus com-\n",
      "position emphasizes formal, structured language, \n",
      "enhancing the model’s capacity for grammatical \n",
      "correctness and nuanced contextual understanding. \n",
      "Given the high domain similarity between the Ko-\n",
      "ELECTRA pre-training corpus and the dataset used \n",
      "in this study, KoELECTRA was selected as the back-\n",
      "bone model, providing a robust foundation for Ko-\n",
      "rean-language entity recognition tasks. \n",
      "3.3 Evaluation Metrics \n",
      "In Named Entity Recognition (NER) tasks, model \n",
      "performance is primarily evaluated using the F1 -\n",
      "score, which represents the harmonic mean of pre-\n",
      "cision and recall, providing a balanced assessment \n",
      "of both error suppression and detection capability \n",
      "[16]. \n",
      "Precision quantifies the proportion of correctly \n",
      "identified entity mentions among all entity predic-\n",
      "tions made by the model. It reflects the model's abil-\n",
      "ity to minimize false positives. \n",
      "Recall measures the proportion of actual entity \n",
      "mentions that the model correctly identifies, indi-\n",
      "cating the model's sensitivity and its ability to min-\n",
      "imize false negatives. \n",
      "The F1-score is calculated as the harmonic mean \n",
      "of precision and recall, providing a single metric \n",
      "that captures the trade -off between these two as-\n",
      "pects. \n",
      "4 EXPERIMENTAL RESULTS \n",
      "4.1 Experimental Environment \n",
      "Model training was performed with a batch size of \n",
      "32, a maximum sequence length of 128 tokens, a \n",
      "learning rate of 2 × 10⁻⁵, and a maximum of 100 \n",
      "epochs. To mitigate overfitting a dropout rate of 0.1 \n",
      "was applied, and the AdamW optimizer was used \n",
      "for parameter updates. An early stopping strategy \n",
      "was incorporated based on validation loss with a \n",
      "patience threshold of three epochs. \n",
      "To analyze the impact of dataset size and entity \n",
      "label distribution on model performance, three da-\n",
      "taset configurations were designed: \n",
      "Balanced: Reduces overrepresented entity cate-\n",
      "gories to alleviate class imbalance while preserving \n",
      "overall dataset size. \n",
      "Original Distribution : Maintains the natural dis-\n",
      "tribution of filtered entities in the dataset. \n",
      "Uniform Small : Enforces an equal number of \n",
      "samples for each entity category to facilitate fair \n",
      "comparative evaluation. \n",
      "All datasets were partitioned into training, vali-\n",
      "dation, and test sets using stratified sampling with \n",
      "an 80:10:10 split ratio to preserve entity distribu-\n",
      "tion across subsets. \n",
      "For text preprocessing, tokenization was per-\n",
      "formed using the KoELECTRA WordPiece tokenizer, \n",
      "which segments input sentences into subword units \n",
      "to effectively handle out-of-vocabulary expressions, \n",
      "including neologisms. Entity annotations followed \n",
      "the standard BIO tagging scheme. The detailed da-\n",
      "taset statistics for each configuration are summa-\n",
      "rized in Table 2. \n",
      "4.2 Results \n",
      "Table 3 presents the F1 -scores obtained by the \n",
      "proposed NER model across three dataset \n",
      "configurations: Original Distribution, Balanced, and \n",
      "Uniform Small. Both macro -averaged and micro -\n",
      "averaged F1 -scores are reported to provide a \n",
      "comprehensive evaluation of overall performance. \n",
      "Fig. 1. ELECTRA pre-training architecture \n",
      " \n",
      "Table 2. Overview of Dataset Composition \n",
      "s\n",
      "The experimental results provide several key \n",
      "insights into the influence of dataset composition \n",
      "and distribution on the performance of deep \n",
      "learning-based NER models, particularly for low -\n",
      "resource entity types such as neologisms. \n",
      "First, the model demonstrated strong \n",
      "performance in the Original Distribution \n",
      "configuration, especially for dominant categories \n",
      "such as Civilization (CV) and Organization (OG), \n",
      "where the availability of extensive training data \n",
      "contributed to high F1 -scores of 0.92 and 0.91, \n",
      "respectively. This observation aligns with well -\n",
      "established findings in NER research that model \n",
      "performance correlates positively with sample \n",
      "abundance and linguistic diversity within the \n",
      "training set. However, this performance advantage  \n",
      "for high -frequency entities comes at the cost of \n",
      "recognition fairness, as reflected in the relatively \n",
      "lower F1 -score of 0.76 for the Theory/Rule (TR) \n",
      "category, which suffers from data scarcity. \n",
      "Second, the Balanced Dataset results highlight \n",
      "the trade -off between absolute performance and \n",
      "class-wise consistency. By limiting the maximum \n",
      "sample size per entity type to 100,000, the model \n",
      "exhibited stable performance across categories, \n",
      "achieving compar able F1 -scores for both high -\n",
      "resource and low-resource entities. The Neologism \n",
      "(NW) category, in particular, achieved a notable F1-\n",
      "score of 0.91, suggesting that the proposed model \n",
      "effectively generalizes to lexically diverse and \n",
      "context-sensitive expressions even under balanced \n",
      "data conditions. These results indicate that \n",
      "mitigating class imbalance enhances overall model \n",
      "robustness, addressing limitations commonly \n",
      "observed in real -world NER applications where \n",
      "entity frequency distributions are highly skewed. \n",
      "In contrast, the Uniform Small Dataset \n",
      "configuration, which imposed strict sample count \n",
      "uniformity across all entity types, revealed \n",
      "limitations in the model's ability to maintain high \n",
      "recognition performance, particularly for \n",
      "previously dominant categorie s. The decline in F1 -\n",
      "scores for OG, AF, and TM to 0.77, 0.70, and 0.74, \n",
      "respectively, underscores the critical role of data \n",
      "volume in enabling accurate entity recognition, \n",
      "especially for linguistically complex or syntactically \n",
      "variable expressions. Interes tingly, the NW \n",
      "category achieved its highest F1-score of 0.92 under \n",
      "this setting, which, upon closer examination, is \n",
      "likely attributable to reduced data diversity rather \n",
      "than true generalization improvement. This result \n",
      "suggests that performance gains in l ow-resource \n",
      "conditions may not directly translate to real -world \n",
      "robustness, highlighting the need for careful dataset \n",
      "design that balances diversity and fairness. \n",
      "Overall, these findings emphasize the \n",
      "importance of dataset composition in optimizing \n",
      "NER performance, particularly for specialized \n",
      "categories such as neologisms, which are \n",
      "characterized by high variability, short life cycles, \n",
      "and low -frequency occurrence.  The results also \n",
      "validate the proposed model's ability to adapt to \n",
      "both imbalanced and balanced data environments, \n",
      "demonstrating practical applicability in real -world \n",
      "scenarios where linguistic resources are often \n",
      "unevenly distributed. \n",
      "In future work, incorporating data augmentation \n",
      "techniques, semi-supervised learning, or continual \n",
      "learning frameworks may further enhance \n",
      "recognition performance for low -resource entity \n",
      "types while preserving fairness across diverse \n",
      "linguistic categories. \n",
      "5 CONCLUSION \n",
      "In this paper, we  proposed a deep learning -based \n",
      "NER model capable of effectively identifying neolo-\n",
      "gisms, which have traditionally posed significant \n",
      "challenges for dictionary - or rule -based detection \n",
      "approaches. By leveraging context -aware learning \n",
      "mechanisms, the model demonstrated robust \n",
      "recognition performance for neologisms character-\n",
      "ized by lexical diversity, short life cycles, and con-\n",
      "text dependency —properties that hinder tradi-\n",
      "tional linguistic resources from timely adaptation. \n",
      "Furthermore, the model exhibited stable prediction \n",
      "performance across variou s entity types, confirm-\n",
      "ing its applicability beyond formal domains such as \n",
      "news and media, and extending to dynamic envi-\n",
      "ronments where neologisms frequently emerge, in-\n",
      "cluding everyday language use and online platforms. \n",
      "The experimental results validate the potential \n",
      "of our proposed model as a technical foundation for \n",
      "enhancing neologism detection systems, thereby \n",
      "contributing to the timely recognition of evolving \n",
      "Table 3. F1-Scores by Entity Type Across Configurations\n",
      "linguistic expressions. Such advancements are cru-\n",
      "cial for narrowing generational language gaps and \n",
      "improving equitable access to information in a rap-\n",
      "idly evolving linguistic landscape. \n",
      "Future research directions include extending the \n",
      "model to handle informal, conversational, and col-\n",
      "loquial language, where neologisms tend to emerge \n",
      "most actively and unpredictably. Adapting the \n",
      "model for application in online communities, social \n",
      "media interactions, and daily interpersonal commu-\n",
      "nication will be essential to ensure practical, real -\n",
      "world effectiveness. Moreover, incorporating tech-\n",
      "niques such as domain adaptation, continual learn-\n",
      "ing, or semi-supervised approaches may further en-\n",
      "hance the model's  ability to generalize to diverse \n",
      "linguistic settings.  \n",
      "REFERENCES \n",
      "[1] Korea Research, “2025 Generation Perception and \n",
      "Intergenerational Conflict Survey: Weekly Report \n",
      "No. 322 -1,” HR Consulting Opinion, Mar. 26, 2025. \n",
      "[Online]. Availa-\n",
      "ble: https://hrcopinion.co.kr/en/archives/32460. \n",
      "[2] Z.R. Sultanova, “Neologisms in the Modern World: \n",
      "Language That Changes Reality,”  Pedagogik \n",
      "Tadqiqotlar Jurnali, vol. 3, no. 1, pp. 188–193, 2025. \n",
      "[Online]. Available:  http://www.wosjour-\n",
      "nals.com/index.php/ptj/article/view/991. \n",
      "[3] I. Aziza and Z. Aktamova, “Modern Neologisms and \n",
      "Their Impact on Society,”  International Journal of \n",
      "Multidisciplinary Research and Growth Evaluation , \n",
      "vol. 6, no. 2, pp. 491 –494, 2025. [Online]. Availa-\n",
      "ble: https://interoncof.com/index.php/USA/arti-\n",
      "cle/view/9830. \n",
      "[4] S.T. Vacalares, A.F.R. Salas, B.J.S. Babac, A.L. Cagala-\n",
      "wan, and C.D. Calimpong, “The Intelligibility of In-\n",
      "ternet Slangs Between Millennials and Gen Zers: A \n",
      "Comparative Study,”  International Journal of Sci-\n",
      "ence and Research Archive, vol. 9, no. 1, pp. 400 –\n",
      "409, 2023, doi: 10.30574/ijsra.2023.9.1.0456. \n",
      "[5] K.I. Nam, “New Words of 2019 Investigation,” Na-\n",
      "tional Institute of Korean Language, Seoul, Dec. \n",
      "2019. [Online]. Available:  https://www.ko-\n",
      "rean.go.kr/front/reportData/reportData-\n",
      "View.do?mn_id=207&report_seq=1104. \n",
      "[6] National Information Society Agency, “AI Hub: \n",
      "Large Web Data -Based Korean Corpus Dataset,” AI \n",
      "Hub, 2022. [Online]. Available:  https://ai-\n",
      "hub.or.kr/aihub-\n",
      "data/data/view.do?currMenu=115&top-\n",
      "Menu=100&aihubDataSe=data&dataSetSn=624. \n",
      "[7] J. Park, “KoELECTRA: Pretrained ELECTRA Model \n",
      "for Korean,” GitHub repository, 2020. [Online]. \n",
      "Available: https://github.com/monologg/KoELEC-\n",
      "TRA. \n",
      "[8] K. Clark, M. -T. Luong, Q.V. Le, and C.D. Manning, \n",
      "“ELECTRA: Pre-Training Text Encoders as Discrim-\n",
      "inators Rather Than Generators,” arXiv preprint, \n",
      "arXiv:2003.10555, 2020, doi: \n",
      "10.48550/arXiv.2003.10555. \n",
      "[9] J.H. Kim, H.C. Kim, and Y.S. Choi, “Feature Generation \n",
      "of Dictionary for Named -Entity Recognition Based \n",
      "on Machine Learning,” Journal of Information Man-\n",
      "agement, vol. 41, no. 2, pp. 31 –46, 2010, doi: \n",
      "10.1633/JIM.2010.41.2.031. \n",
      "[10] V. Yadav and S. Bethard, “A Survey on Recent Ad-\n",
      "vances in Named Entity Recognition From Deep \n",
      "Learning Models,” arXiv preprint arXiv:1910.11470, \n",
      "2019, doi: 10.48550/arXiv.1910.11470. \n",
      "[11] J. Li, A. Sun, J. Han and C. Li, “A Survey on Deep \n",
      "Learning for Named Entity Recognition,”  IEEE \n",
      "Transactions on Knowledge and Data Engineering, \n",
      "vol. 34, no. 1, pp. 50 –70, Jan. 2022, doi: \n",
      "10.1109/TKDE.2020.2981314. \n",
      "[12] Z. Huang, W. Xu, and K. Yu, “Bidirectional LSTM-CRF \n",
      "Models for Sequence Tagging,” arXiv preprint \n",
      "arXiv:1508.01991, 2015, doi: \n",
      "10.48550/arXiv.1508.01991. \n",
      "[13] M. Munnangi, “A Brief History of Named Entity \n",
      "Recognition,” arXiv preprint, arXiv:2411.05057, \n",
      "2024, doi: 10.48550/arXiv.2411.05057. \n",
      "[14] Y. Li, “Sources of English Neologisms,”  Linguistics \n",
      "and Education, vol. 42, no. 1, pp. 210–215, 2024, doi: \n",
      "10.54254/2753-7048/42/20240854. \n",
      "[15] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: \n",
      "Pre-Training of Deep Bidirectional Transformers \n",
      "for Language Understanding,” arXiv preprint \n",
      "arXiv:1810.04805, 2018, doi: \n",
      "10.48550/arXiv.1810.04805. \n",
      "[16] A. Mansouri, L.S. Affendey, and A. Mamat, “Named \n",
      "Entity Recognition Approaches,”  Int. J. Comput. Sci. \n",
      "Netw. Security, vol. 8, no. 2, pp. 339–344, Feb. 2008. \n",
      "[Online]. Available:  http://pa-\n",
      "per.ijcsns.org/07_book/200802/20080246.pdf.  \n",
      " \n",
      "Junhaeng Lee is a fourth-year undergraduate computer engineer-\n",
      "ing student at Sungkyul University in Anyang, South Korea. His cur-\n",
      "rent research interest is natural language processing. \n",
      " \n",
      "HyunYoung Oh is a third-year undergraduate computer engi-neer-\n",
      "ing student at Sungkyul University in Anyang, South Korea. His cur-\n",
      "rent research interest is natural language processing and explaina-\n",
      "ble AI. \n",
      " \n",
      "SangWoo Park is a third -year undergraduate computer engineer-\n",
      "ing student at Sungkyul University in Anyang, South Korea. His cur-\n",
      "rent research interest is natural language processing. \n",
      " \n",
      "Young-myoung Kang is currently an assistant professor in \n",
      "the Dept. of Computer Engineering at Sungkyul University, \n",
      "Anyang, Korea. He received the Ph.D. degree at Seoul Nation- \n",
      "al University in 2013.\n"
     ]
    }
   ],
   "source": [
    "for doc in docs[:10]:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "83923493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 32.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loader2 = PyMuPDFLoader(FILE_PATH)\n",
    "\n",
    "docs2 = loader2.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0e45a187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-Aware Named Entity Recognition \n",
      "for Neologism Detection: A Deep Learning \n",
      "Approach to Bridging Generational \n",
      "Language Gaps \n",
      "Junhaeng Lee, HyunYoung Oh, SangWoo Park, and Young-myoung Kang* \n",
      "Abstract—The rapid proliferation of neologisms in online and social communication has intensified linguistic \n",
      "disparities across generations, posing challenges for timely and accurate neologism detection. Conventional \n",
      "dictionary- and rule-based methods struggle to capture the dynamic, context-dependent nature of evolving \n",
      "expressions, limiting their practical applicability. This study proposes a deep learning-based, context-aware \n",
      "Named Entity Recognition (NER) model designed to effectively identify neologisms and mitigate generational \n",
      "language gaps. The model leverages KoELECTRA, a pre-trained Korean language model optimized for \n",
      "contextual understanding, and is evaluated on large-scale datasets with varying entity distributions. \n",
      "Experimental results demonstrate that the proposed model achieves robust neologism detection \n",
      "performance, with an F1-score of 0.89 under real-world imbalanced conditions and up to 0.92 in controlled, \n",
      "uniform settings. Furthermore, the model exhibits balanced recognition across diverse entity types, \n",
      "confirming its applicability to both formal and informal language environments. These findings highlight the \n",
      "model's potential as a scalable solution for enhancing linguistic accessibility and supporting real-time \n",
      "adaptation to evolving language, particularly in bridging generational communication gaps. \n",
      "Research Keywords—Deep Learning, Generational Communication, Korean Neologisms, Named Entity \n",
      "Recognition, Natural Language Processing. \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――― \n",
      "1 INTRODUCTION\n",
      "In contemporary society, conflicts driven by na-\n",
      "tional, racial, religious, and generational divides are \n",
      "escalating globally. Among these, generational con-\n",
      "flict has become a critical social concern in South \n",
      "Korea. According to a 2025 national survey, nearly \n",
      "80% of respondents identified generational conflict \n",
      "as a serious issue, primarily attributing it to com-\n",
      "munication barriers and collaboration difficulties \n",
      "across age groups [1]. Notably, these challenges ex-\n",
      "tend beyond differences in value systems, originat-\n",
      "ing from fundamental discrepancies in linguistic ex-\n",
      "pression patterns and communication modalities \n",
      "between generations. \n",
      "The proliferation of internet-based platforms \n",
      "and social media has significantly accelerated the \n",
      "emergence and diffusion of neologisms, which now \n",
      "serve as integral components of everyday commu-\n",
      "nication, transcending conventional generational \n",
      "boundaries [2]. In the South Korean context, neolo-\n",
      "gisms are extensively utilized across mass media, \n",
      "broadcasting, and public discourse, functioning as \n",
      "key linguistic elements that shape socio-cultural in-\n",
      "teraction [3]. However, intergenerational dispari-\n",
      "ties in the recognition, comprehension, and utiliza-\n",
      "tion of neologisms persist, exacerbating inequalities \n",
      "not only in the transmission but also in the interpre-\n",
      "tation and accessibility of information [4]. Conven-\n",
      "tional approaches to neologism detection primarily \n",
      "rely on static dictionary-based methodologies, \n",
      "———————————————— \n",
      "• Junhaeng Lee is with the Department of Computer Engineer-\n",
      "ing, Sungkyul University, Anyang, Korea. E-mail: \n",
      "hkhk9495@sunkyul.ac.kr. \n",
      "• HyunYoung Oh is with the Department of Computer Engineer-\n",
      "ing, Sungkyul University, Anyang, Korea. E-mail: \n",
      "ohy414@sungkyul.ac.kr. \n",
      "• SangWoo Park is with the Department of Computer Engineer-\n",
      "ing, Sungkyul University, Anyang, Korea. E-mail: \n",
      "sgpswoo@sungkyul.ac.kr. \n",
      "• Young-myoung Kang (corresponding author) is with the De-\n",
      "partment of Computer Engineering, Sungkyul University, An-\n",
      "yang, Korea. E-mail: ykang@sungkyul.ac.kr.\n",
      "where unregistered word forms are identified by \n",
      "comparing word frequencies within a target corpus \n",
      "against pre-defined lexicons [5]. While such meth-\n",
      "ods are effective in detecting neologisms with high \n",
      "linguistic stability, they exhibit significant limita-\n",
      "tions in capturing transient, context-sensitive, and \n",
      "rapidly evolving expressions characteristic of dy-\n",
      "namic online language environments. Moreover, \n",
      "these approaches lack adaptability to account for \n",
      "the real-time linguistic innovation cycles prevalent \n",
      "in digital communication spaces. \n",
      "This study defines two core limitations underly-\n",
      "ing existing methodologies: (1) the inability to ac-\n",
      "commodate the temporal volatility and context de-\n",
      "pendency inherent in neologism generation and \n",
      "dissemination; and (2) the amplification of inter-\n",
      "generational linguistic asymmetries, which impairs \n",
      "effective communication and perpetuates dispari-\n",
      "ties in information accessibility. To overcome these \n",
      "limitations, we propose a deep learning-based \n",
      "Named Entity Recognition (NER) framework opti-\n",
      "mized for the detection of neologisms within con-\n",
      "text-sensitive environments. The proposed model is \n",
      "trained on a large-scale Korean web corpus [6], pre-\n",
      "dominantly composed of news media content, and \n",
      "annotated with seven entity categories: Organiza-\n",
      "tion (OG), Artifact (AF), Civilization (CV), Field of \n",
      "Study (FD), Theory/Rule (TR), Term (TM), and Ne-\n",
      "ologism (NW). The integration of neologism detec-\n",
      "tion within a multi-entity recognition architecture \n",
      "enhances the model’s generalizability and practical \n",
      "applicability across diverse linguistic settings, con-\n",
      "tributing to the mitigation of generational commu-\n",
      "nication gaps. \n",
      "To evaluate the robustness and performance \n",
      "consistency of the proposed model under varying \n",
      "data distributions, three dataset variants were con-\n",
      "structed: Original, Balanced, and Uniform. The \n",
      "model architecture employs KoELECTRA [7], a Ko-\n",
      "rean language-adapted version of the Transformer-\n",
      "based ELECTRA framework [8], pre-trained primar-\n",
      "ily on news corpora to optimize the recognition of \n",
      "structured linguistic patterns and context-depend-\n",
      "ent expressions. Experimental results indicate that \n",
      "the proposed model achieves a macro-averaged F1-\n",
      "score of 0.88 and a neologism-specific F1-score of \n",
      "0.91 under the Balanced dataset configuration, \n",
      "demonstrating superior performance in detecting \n",
      "evolving and contextually grounded linguistic \n",
      "forms. These findings validate the model’s capacity \n",
      "to address the limitations of conventional diction-\n",
      "ary- or rule-based approaches and highlight its po-\n",
      "tential to facilitate intergenerational linguistic inte-\n",
      "gration, thereby promoting equitable information \n",
      "access and enhancing efficiency in social communi-\n",
      "cation. \n",
      "2 RELATED WORK \n",
      "2.1 Neologism Collection \n",
      "Traditional approaches to neologism collection \n",
      "have primarily relied on extracting candidate terms \n",
      "from various online platforms, including internet \n",
      "communities, blogs, social media, and collaborative \n",
      "knowledge bases such as Wikipedia [4]. In this pro-\n",
      "cess, words that exhibited low frequency within the \n",
      "collected corpus and were absent from existing dic-\n",
      "tionaries were flagged as potential neologisms. \n",
      "These candidates were then compared against cu-\n",
      "rated lexicons comprising well-defined terms to fi-\n",
      "nalize the neologism list [9]. Through iterative re-\n",
      "finement, existing dictionaries were either ex-\n",
      "panded or updated to incorporate newly identified \n",
      "expressions. \n",
      "However, neologisms in contemporary digital \n",
      "environments exhibit inherently unstable life cycles, \n",
      "characterized by rapid emergence, dissemination, \n",
      "and disappearance. Their meanings and usage pat-\n",
      "terns are highly fluid, often varying across \n",
      "timeframes, online subcultures, and communities. \n",
      "Consequently, static lexicon- or rule-based methods \n",
      "struggle to effectively capture this linguistic volatil-\n",
      "ity. Furthermore, neologisms frequently manifest in \n",
      "diverse forms—including low-frequency tokens, in-\n",
      "tentional misspellings, abbreviations, hybrid struc-\n",
      "tures, and combinations with emojis—complicating \n",
      "their detection through standardized pattern recog-\n",
      "nition techniques. The semantic ambiguity or poly-\n",
      "semy associated with many neologisms presents \n",
      "additional challenges for deep learning models, par-\n",
      "ticularly concerning the acquisition of sufficient, \n",
      "high-quality annotated data required for accurate \n",
      "embedding learning [9]. \n",
      "2.2 Named Entity Recognition \n",
      "Named Entity Recognition (NER) is a fundamental \n",
      "subtask in natural language processing (NLP) that \n",
      "involves identifying and classifying entities within \n",
      "unstructured text, including persons, locations, or-\n",
      "ganizations, temporal expressions, and domain-\n",
      "specific entities such as chemical compounds or \n",
      "pharmaceutical products [10]. \n",
      "Initial research in NER predominantly utilized \n",
      "rule-based approaches, leveraging handcrafted se-\n",
      "mantic, syntactic, and pattern-based rules in con-\n",
      "junction with domain-specific dictionaries. While \n",
      "these methods achieved high precision within nar-\n",
      "row domains, they suffered from limited scalability\n",
      "and low recall, primarily due to the incompleteness \n",
      "of lexicons and the rigidity of manually crafted rules \n",
      "[11]. \n",
      "The advent of deep learning has significantly ad-\n",
      "vanced NER methodologies, with neural network-\n",
      "based models demonstrating superior performance \n",
      "over traditional rule-based systems [11]. In partic-\n",
      "ular, the Bidirectional Long Short-Term Memory \n",
      "with Conditional Random Fields (BiLSTM-CRF) ar-\n",
      "chitecture has become a widely adopted framework, \n",
      "effectively capturing bidirectional contextual de-\n",
      "pendencies within sentences and improving se-\n",
      "quential labeling accuracy [12]. \n",
      "More recently, the introduction of pre-trained \n",
      "language models and Transformer-based architec-\n",
      "tures has revolutionized NER by enabling context-\n",
      "aware representation learning without the need for \n",
      "manual feature engineering. These models, trained \n",
      "on large-scale corpora, acquire generalized linguis-\n",
      "tic knowledge that facilitates high-performance \n",
      "NER, even in scenarios with limited labeled data \n",
      "[13]. Their ability to effectively capture complex se-\n",
      "mantic relationships has significantly improved \n",
      "NER accuracy across diverse languages and do-\n",
      "mains. \n",
      "3 METHODOLOGY \n",
      "3.1 Data set \n",
      "The dataset utilized in this study is a large-scale, \n",
      "web-based \n",
      "Korean \n",
      "corpus \n",
      "comprising \n",
      "approximately one billion words, released in 2021. \n",
      "The corpus primarily consists of news articles \n",
      "spanning 17 thematic categories, including society, \n",
      "culture, science, and economics. To facilitate entity \n",
      "recognition tasks, the dataset was annotated with \n",
      "seven entity types: Organization (OG), Artifact (AF), \n",
      "Civilization (CV), Field of Study (FD), Theory/Rule \n",
      "(TR), Term (TM), and Neologism (NW). The \n",
      "annotation process was conducted using an \n",
      "automated authoring tool to ensure large-scale \n",
      "consistency. For neologism tagging, only newly \n",
      "emerging terms that appeared with a certain \n",
      "frequency but were absent from standardized \n",
      "Korean dictionaries were labeled accordingly. \n",
      "Given the inherent ambiguity and inconsistency \n",
      "in the definition of neologisms in prior works, this \n",
      "study established explicit classification criteria to \n",
      "ensure consistency and clarity in neologism \n",
      "identification. Specifically, neologisms were strictly \n",
      "limited to the following categories: \n",
      "• \n",
      "Compound expressions formed by the combi-\n",
      "nation of two or more existing words, \n",
      "• \n",
      "Hybrid terms resulting from partial word \n",
      "combinations or morphological blending, \n",
      "• \n",
      "Loanwords or semantic extensions of pre-ex-\n",
      "isting terms introduced into Korean. \n",
      "Any expressions falling outside these categories \n",
      "were excluded from the neologism label set [14]. To \n",
      "maintain alignment between the refined neologism \n",
      "criteria and the overall dataset structure, a \n",
      "proportional downsampling strategy was applied, \n",
      "ensuring that the entity distributions within the \n",
      "dataset \n",
      "reflected \n",
      "the \n",
      "updated \n",
      "classification \n",
      "boundaries. \n",
      "An analysis of the dataset revealed significant \n",
      "distributional imbalance across entity types. In par-\n",
      "ticular, Organization (OG) and Civilization (CV) en-\n",
      "tities dominated the corpus, which is attributable to \n",
      "the corpus's content composition, heavily focused \n",
      "on socio-cultural narratives, government entities, \n",
      "corporate organizations, and popular culture. Con-\n",
      "versely, entities associated with specialized or do-\n",
      "main-specific expressions, such as Neologism (NW) \n",
      "and Theory/Rule (TR), were comparatively scarce, \n",
      "reflecting their lower natural occurrence within \n",
      "mainstream media texts. Table 1 presents a sum-\n",
      "mary of the entity type distribution, illustrating \n",
      "both the original and post-filtering instance counts \n",
      "for each category. \n",
      "These distributional patterns highlight the chal-\n",
      "lenges associated with low-resource entity types \n",
      "such as neologisms, underscoring the importance of \n",
      "developing robust models capable of accurately de-\n",
      "tecting rare and context-sensitive expressions \n",
      "within imbalanced datasets. \n",
      "3.2 Model \n",
      "The widely used BERT model [15] conducts pre-\n",
      "training via Masked Language Modeling (MLM) \n",
      "which suffers from suboptimal data utilization and \n",
      "distributional mismatch between pre-training and \n",
      "fine-tuning [8]. \n",
      "To address these limitations, the ELECTRA \n",
      "model introduces a more efficient pre-training par-\n",
      "adigm known as Replaced Token Detection (RTD). \n",
      "Table 1. Named Entities Before and After Filtering\n",
      "ELECTRA employs a dual-Transformer architecture \n",
      "consisting of a Generator and a Discriminator. The \n",
      "Generator produces plausible replacements for ran-\n",
      "domly selected tokens, while the Discriminator per-\n",
      "forms token-level binary classification to distin-\n",
      "guish between original and replaced tokens. Unlike \n",
      "MLM, this approach eliminates the need for artifi-\n",
      "cial mask tokens, preserving the natural contextual \n",
      "flow of input sequences while enabling loss compu-\n",
      "tation across all tokens. As a result, ELECTRA \n",
      "achieves superior learning efficiency and improved \n",
      "alignment between pre-training and downstream \n",
      "tasks. The overall model architecture and training \n",
      "mechanism are depicted in Fig. 1. \n",
      "In this study, we adopted KoELECTRA, a Korean \n",
      "language-optimized variant of ELECTRA. KoELEC-\n",
      "TRA was pre-trained on a large-scale Korean corpus \n",
      "comprising approximately 14 GB of text, including \n",
      "diverse sources such as news articles, encyclopedic \n",
      "entries, and Namu Wiki content. This corpus com-\n",
      "position emphasizes formal, structured language, \n",
      "enhancing the model’s capacity for grammatical \n",
      "correctness and nuanced contextual understanding. \n",
      "Given the high domain similarity between the Ko-\n",
      "ELECTRA pre-training corpus and the dataset used \n",
      "in this study, KoELECTRA was selected as the back-\n",
      "bone model, providing a robust foundation for Ko-\n",
      "rean-language entity recognition tasks. \n",
      "3.3 Evaluation Metrics \n",
      "In Named Entity Recognition (NER) tasks, model \n",
      "performance is primarily evaluated using the F1-\n",
      "score, which represents the harmonic mean of pre-\n",
      "cision and recall, providing a balanced assessment \n",
      "of both error suppression and detection capability \n",
      "[16]. \n",
      "Precision quantifies the proportion of correctly \n",
      "identified entity mentions among all entity predic-\n",
      "tions made by the model. It reflects the model's abil-\n",
      "ity to minimize false positives. \n",
      "Recall measures the proportion of actual entity \n",
      "mentions that the model correctly identifies, indi-\n",
      "cating the model's sensitivity and its ability to min-\n",
      "imize false negatives. \n",
      "The F1-score is calculated as the harmonic mean \n",
      "of precision and recall, providing a single metric \n",
      "that captures the trade-off between these two as-\n",
      "pects. \n",
      "4 EXPERIMENTAL RESULTS \n",
      "4.1 Experimental Environment \n",
      "Model training was performed with a batch size of \n",
      "32, a maximum sequence length of 128 tokens, a \n",
      "learning rate of 2 × 10⁻⁵, and a maximum of 100 \n",
      "epochs. To mitigate overfitting a dropout rate of 0.1 \n",
      "was applied, and the AdamW optimizer was used \n",
      "for parameter updates. An early stopping strategy \n",
      "was incorporated based on validation loss with a \n",
      "patience threshold of three epochs. \n",
      "To analyze the impact of dataset size and entity \n",
      "label distribution on model performance, three da-\n",
      "taset configurations were designed: \n",
      "Balanced: Reduces overrepresented entity cate-\n",
      "gories to alleviate class imbalance while preserving \n",
      "overall dataset size. \n",
      "Original Distribution: Maintains the natural dis-\n",
      "tribution of filtered entities in the dataset. \n",
      "Uniform Small: Enforces an equal number of \n",
      "samples for each entity category to facilitate fair \n",
      "comparative evaluation. \n",
      "All datasets were partitioned into training, vali-\n",
      "dation, and test sets using stratified sampling with \n",
      "an 80:10:10 split ratio to preserve entity distribu-\n",
      "tion across subsets. \n",
      "For text preprocessing, tokenization was per-\n",
      "formed using the KoELECTRA WordPiece tokenizer, \n",
      "which segments input sentences into subword units \n",
      "to effectively handle out-of-vocabulary expressions, \n",
      "including neologisms. Entity annotations followed \n",
      "the standard BIO tagging scheme. The detailed da-\n",
      "taset statistics for each configuration are summa-\n",
      "rized in Table 2. \n",
      "4.2 Results \n",
      "Table 3 presents the F1-scores obtained by the \n",
      "proposed \n",
      "NER \n",
      "model \n",
      "across \n",
      "three \n",
      "dataset \n",
      "configurations: Original Distribution, Balanced, and \n",
      "Uniform Small. Both macro-averaged and micro-\n",
      "averaged F1-scores are reported to provide a \n",
      "comprehensive evaluation of overall performance. \n",
      "Fig. 1. ELECTRA pre-training architecture \n",
      "Table 2. Overview of Dataset Composition\n",
      "The experimental results provide several key \n",
      "insights into the influence of dataset composition \n",
      "and distribution on the performance of deep \n",
      "learning-based NER models, particularly for low-\n",
      "resource entity types such as neologisms. \n",
      "First, \n",
      "the \n",
      "model \n",
      "demonstrated \n",
      "strong \n",
      "performance \n",
      "in \n",
      "the \n",
      "Original \n",
      "Distribution \n",
      "configuration, especially for dominant categories \n",
      "such as Civilization (CV) and Organization (OG), \n",
      "where the availability of extensive training data \n",
      "contributed to high F1-scores of 0.92 and 0.91, \n",
      "respectively. This observation aligns with well-\n",
      "established findings in NER research that model \n",
      "performance correlates positively with sample \n",
      "abundance and linguistic diversity within the \n",
      "training set. However, this performance advantage \n",
      "for high-frequency entities comes at the cost of \n",
      "recognition fairness, as reflected in the relatively \n",
      "lower F1-score of 0.76 for the Theory/Rule (TR) \n",
      "category, which suffers from data scarcity. \n",
      "Second, the Balanced Dataset results highlight \n",
      "the trade-off between absolute performance and \n",
      "class-wise consistency. By limiting the maximum \n",
      "sample size per entity type to 100,000, the model \n",
      "exhibited stable performance across categories, \n",
      "achieving comparable F1-scores for both high-\n",
      "resource and low-resource entities. The Neologism \n",
      "(NW) category, in particular, achieved a notable F1-\n",
      "score of 0.91, suggesting that the proposed model \n",
      "effectively generalizes to lexically diverse and \n",
      "context-sensitive expressions even under balanced \n",
      "data conditions. These results indicate that \n",
      "mitigating class imbalance enhances overall model \n",
      "robustness, \n",
      "addressing \n",
      "limitations \n",
      "commonly \n",
      "observed in real-world NER applications where \n",
      "entity frequency distributions are highly skewed. \n",
      "In \n",
      "contrast, \n",
      "the \n",
      "Uniform \n",
      "Small \n",
      "Dataset \n",
      "configuration, which imposed strict sample count \n",
      "uniformity across all entity types, revealed \n",
      "limitations in the model's ability to maintain high \n",
      "recognition \n",
      "performance, \n",
      "particularly \n",
      "for \n",
      "previously dominant categories. The decline in F1-\n",
      "scores for OG, AF, and TM to 0.77, 0.70, and 0.74, \n",
      "respectively, underscores the critical role of data \n",
      "volume in enabling accurate entity recognition, \n",
      "especially for linguistically complex or syntactically \n",
      "variable \n",
      "expressions. \n",
      "Interestingly, \n",
      "the \n",
      "NW \n",
      "category achieved its highest F1-score of 0.92 under \n",
      "this setting, which, upon closer examination, is \n",
      "likely attributable to reduced data diversity rather \n",
      "than true generalization improvement. This result \n",
      "suggests that performance gains in low-resource \n",
      "conditions may not directly translate to real-world \n",
      "robustness, highlighting the need for careful dataset \n",
      "design that balances diversity and fairness. \n",
      "Overall, \n",
      "these \n",
      "findings \n",
      "emphasize \n",
      "the \n",
      "importance of dataset composition in optimizing \n",
      "NER performance, particularly for specialized \n",
      "categories \n",
      "such \n",
      "as \n",
      "neologisms, \n",
      "which \n",
      "are \n",
      "characterized by high variability, short life cycles, \n",
      "and low-frequency occurrence. The results also \n",
      "validate the proposed model's ability to adapt to \n",
      "both imbalanced and balanced data environments, \n",
      "demonstrating practical applicability in real-world \n",
      "scenarios where linguistic resources are often \n",
      "unevenly distributed. \n",
      "In future work, incorporating data augmentation \n",
      "techniques, semi-supervised learning, or continual \n",
      "learning \n",
      "frameworks \n",
      "may \n",
      "further \n",
      "enhance \n",
      "recognition performance for low-resource entity \n",
      "types while preserving fairness across diverse \n",
      "linguistic categories. \n",
      "5 CONCLUSION \n",
      "In this paper, we proposed a deep learning-based \n",
      "NER model capable of effectively identifying neolo-\n",
      "gisms, which have traditionally posed significant \n",
      "challenges for dictionary- or rule-based detection \n",
      "approaches. By leveraging context-aware learning \n",
      "mechanisms, the model demonstrated robust \n",
      "recognition performance for neologisms character-\n",
      "ized by lexical diversity, short life cycles, and con-\n",
      "text dependency—properties that hinder tradi-\n",
      "tional linguistic resources from timely adaptation. \n",
      "Furthermore, the model exhibited stable prediction \n",
      "performance across various entity types, confirm-\n",
      "ing its applicability beyond formal domains such as \n",
      "news and media, and extending to dynamic envi-\n",
      "ronments where neologisms frequently emerge, in-\n",
      "cluding everyday language use and online platforms. \n",
      "The experimental results validate the potential \n",
      "of our proposed model as a technical foundation for \n",
      "enhancing neologism detection systems, thereby \n",
      "contributing to the timely recognition of evolving \n",
      "Table 3. F1-Scores by Entity Type Across Configurations\n",
      "linguistic expressions. Such advancements are cru-\n",
      "cial for narrowing generational language gaps and \n",
      "improving equitable access to information in a rap-\n",
      "idly evolving linguistic landscape. \n",
      "Future research directions include extending the \n",
      "model to handle informal, conversational, and col-\n",
      "loquial language, where neologisms tend to emerge \n",
      "most actively and unpredictably. Adapting the \n",
      "model for application in online communities, social \n",
      "media interactions, and daily interpersonal commu-\n",
      "nication will be essential to ensure practical, real-\n",
      "world effectiveness. Moreover, incorporating tech-\n",
      "niques such as domain adaptation, continual learn-\n",
      "ing, or semi-supervised approaches may further en-\n",
      "hance the model's ability to generalize to diverse \n",
      "linguistic settings.  \n",
      "REFERENCES \n",
      "[1] Korea Research, “2025 Generation Perception and \n",
      "Intergenerational Conflict Survey: Weekly Report \n",
      "No. 322-1,” HR Consulting Opinion, Mar. 26, 2025. \n",
      "[Online]. \n",
      "Availa-\n",
      "ble: https://hrcopinion.co.kr/en/archives/32460. \n",
      "[2] Z.R. Sultanova, “Neologisms in the Modern World: \n",
      "Language \n",
      "That \n",
      "Changes \n",
      "Reality,” Pedagogik \n",
      "Tadqiqotlar Jurnali, vol. 3, no. 1, pp. 188–193, 2025. \n",
      "[Online]. \n",
      "Available: http://www.wosjour-\n",
      "nals.com/index.php/ptj/article/view/991. \n",
      "[3] I. Aziza and Z. Aktamova, “Modern Neologisms and \n",
      "Their Impact on Society,” International Journal of \n",
      "Multidisciplinary Research and Growth Evaluation, \n",
      "vol. 6, no. 2, pp. 491–494, 2025. [Online]. Availa-\n",
      "ble: https://interoncof.com/index.php/USA/arti-\n",
      "cle/view/9830. \n",
      "[4] S.T. Vacalares, A.F.R. Salas, B.J.S. Babac, A.L. Cagala-\n",
      "wan, and C.D. Calimpong, “The Intelligibility of In-\n",
      "ternet Slangs Between Millennials and Gen Zers: A \n",
      "Comparative Study,” International Journal of Sci-\n",
      "ence and Research Archive, vol. 9, no. 1, pp. 400–\n",
      "409, 2023, doi: 10.30574/ijsra.2023.9.1.0456. \n",
      "[5] K.I. Nam, “New Words of 2019 Investigation,” Na-\n",
      "tional Institute of Korean Language, Seoul, Dec. \n",
      "2019. \n",
      "[Online]. \n",
      "Available: https://www.ko-\n",
      "rean.go.kr/front/reportData/reportData-\n",
      "View.do?mn_id=207&report_seq=1104. \n",
      "[6] National Information Society Agency, “AI Hub: \n",
      "Large Web Data-Based Korean Corpus Dataset,” AI \n",
      "Hub, \n",
      "2022. \n",
      "[Online]. \n",
      "Available: https://ai-\n",
      "hub.or.kr/aihub-\n",
      "data/data/view.do?currMenu=115&top-\n",
      "Menu=100&aihubDataSe=data&dataSetSn=624. \n",
      "[7] J. Park, “KoELECTRA: Pretrained ELECTRA Model \n",
      "for Korean,” GitHub repository, 2020. [Online]. \n",
      "Available: https://github.com/monologg/KoELEC-\n",
      "TRA. \n",
      "[8] K. Clark, M.-T. Luong, Q.V. Le, and C.D. Manning, \n",
      "“ELECTRA: Pre-Training Text Encoders as Discrim-\n",
      "inators Rather Than Generators,” arXiv preprint, \n",
      "arXiv:2003.10555, \n",
      "2020, \n",
      "doi: \n",
      "10.48550/arXiv.2003.10555. \n",
      "[9] J.H. Kim, H.C. Kim, and Y.S. Choi, “Feature Generation \n",
      "of Dictionary for Named-Entity Recognition Based \n",
      "on Machine Learning,” Journal of Information Man-\n",
      "agement, vol. 41, no. 2, pp. 31–46, 2010, doi: \n",
      "10.1633/JIM.2010.41.2.031. \n",
      "[10] V. Yadav and S. Bethard, “A Survey on Recent Ad-\n",
      "vances in Named Entity Recognition From Deep \n",
      "Learning Models,” arXiv preprint arXiv:1910.11470, \n",
      "2019, doi: 10.48550/arXiv.1910.11470. \n",
      "[11] J. Li, A. Sun, J. Han and C. Li, “A Survey on Deep \n",
      "Learning for Named Entity Recognition,” IEEE \n",
      "Transactions on Knowledge and Data Engineering, \n",
      "vol. 34, no. 1, pp. 50–70, Jan. 2022, doi: \n",
      "10.1109/TKDE.2020.2981314. \n",
      "[12] Z. Huang, W. Xu, and K. Yu, “Bidirectional LSTM-CRF \n",
      "Models for Sequence Tagging,” arXiv preprint \n",
      "arXiv:1508.01991, \n",
      "2015, \n",
      "doi: \n",
      "10.48550/arXiv.1508.01991. \n",
      "[13] M. Munnangi, “A Brief History of Named Entity \n",
      "Recognition,” arXiv preprint, arXiv:2411.05057, \n",
      "2024, doi: 10.48550/arXiv.2411.05057. \n",
      "[14] Y. Li, “Sources of English Neologisms,” Linguistics \n",
      "and Education, vol. 42, no. 1, pp. 210–215, 2024, doi: \n",
      "10.54254/2753-7048/42/20240854. \n",
      "[15] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: \n",
      "Pre-Training of Deep Bidirectional Transformers \n",
      "for Language Understanding,” arXiv preprint \n",
      "arXiv:1810.04805, \n",
      "2018, \n",
      "doi: \n",
      "10.48550/arXiv.1810.04805. \n",
      "[16] A. Mansouri, L.S. Affendey, and A. Mamat, “Named \n",
      "Entity Recognition Approaches,” Int. J. Comput. Sci. \n",
      "Netw. Security, vol. 8, no. 2, pp. 339–344, Feb. 2008. \n",
      "[Online]. \n",
      "Available: \n",
      "http://pa-\n",
      "per.ijcsns.org/07_book/200802/20080246.pdf.  \n",
      " \n",
      "Junhaeng Lee is a fourth-year undergraduate computer engineer-\n",
      "ing student at Sungkyul University in Anyang, South Korea. His cur-\n",
      "rent research interest is natural language processing. \n",
      " \n",
      "HyunYoung Oh is a third-year undergraduate computer engi-neer-\n",
      "ing student at Sungkyul University in Anyang, South Korea. His cur-\n",
      "rent research interest is natural language processing and explaina-\n",
      "ble AI. \n",
      " \n",
      "SangWoo Park is a third-year undergraduate computer engineer-\n",
      "ing student at Sungkyul University in Anyang, South Korea. His cur-\n",
      "rent research interest is natural language processing. \n",
      " \n",
      "Young-myoung Kang is currently an assistant professor in \n",
      "the Dept. of Computer Engineering at Sungkyul University, \n",
      "Anyang, Korea. He received the Ph.D. degree at Seoul Nation- \n",
      "al University in 2013.\n"
     ]
    }
   ],
   "source": [
    "for doc in docs2:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "88c46810",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len, # 텍스트 길이 계산 함수 지정 \n",
    "    is_separator_regex=False \n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a7f06cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-Aware Named Entity Recognition \n",
      "for Neologism Detection: A Deep Learning \n",
      "Approach to Bridging Generational \n",
      "Language Gaps  \n",
      "Junhaeng Lee,  HyunYoung Oh , SangWoo Park, and Young -myoung Kang*  \n",
      "Abstract—The rapid proliferation of neologisms in online and social communication has intensified\n",
      "contextual understanding,  and is evaluated on large -scale datasets with varying entity distributions. \n",
      "Experimental results demonstrate that the proposed model achieves robust neologism detection \n",
      "performance, with an F1 -score of 0.89 under real -world imbalanced conditions and up to 0.92 in cont\n",
      "Recognition, Natural Language Processing. \n",
      "――――――――――――――――――――――――――――――――――――――――――――――――――――――――― \n",
      "1 INTRODUCTION\n",
      "In contemporary society, conflicts driven by na-\n",
      "tional, racial, religious, and generational divides are \n",
      "escalating globally. Among these, generational con-\n",
      "flict has become a critic\n",
      "The proliferation of internet -based platforms \n",
      "and social media has significantly accelerated the \n",
      "emergence and diffusion of neologisms, which now \n",
      "serve as integral components of everyday commu-\n",
      "nication, transcending conventional generational \n",
      "boundaries [2]. In the South Korean context, neolo-\n",
      "\n",
      "———————————————— \n",
      "• Junhaeng Lee is with the Department of Computer Engineer-\n",
      "ing, Sungkyul University, Anyang, Korea. E-mail: \n",
      "hkhk9495@sunkyul.ac.kr. \n",
      "• HyunYoung Oh is with the Department of Computer Engineer-\n",
      "ing, Sungkyul University, Anyang, Korea. E-mail: \n",
      "ohy414@sungkyul.ac.kr. \n",
      "• SangWoo Par\n",
      "where unregistered word forms are identified by \n",
      "comparing word frequencies within a target corpus \n",
      "against pre-defined lexicons [5]. While such meth-\n",
      "ods are effective in detecting neologisms with high \n",
      "linguistic stability, they exhibit significant limita-\n",
      "tions in capturing transient, context -se\n",
      "generational linguistic asymmetries, which impairs \n",
      "effective communication and perpetuates dispari-\n",
      "ties in information accessibility. To overcome these \n",
      "limitations, we propose a deep learning -based \n",
      "Named Entity Recognition (NER) framework opti-\n",
      "mized for the detection of ne ologisms within con-\n",
      "tributing to the mitigation of generational commu-\n",
      "nication gaps. \n",
      "To evaluate the robustness and performance \n",
      "consistency of the proposed model under varying \n",
      "data distributions, three dataset variants were con-\n",
      "structed: Original, Balanced, and Uniform. The \n",
      "model architecture employs KoELECTRA [ \n",
      "forms. These findings validate the model’s ca pacity \n",
      "to address the limitations of conventional diction-\n",
      "ary- or rule-based approaches and highlight its po-\n",
      "tential to facilitate intergenerational linguistic inte-\n",
      "gration, thereby promoting equitable information \n",
      "access and enhancing efficiency in \n",
      "rated lexicons comprising well -defined terms to fi-\n",
      "nalize the neol ogism list [ 9]. Through iterative re-\n",
      "finement, existing dictionaries were either ex-\n",
      "panded or updated to incorporate newly identified \n",
      "expressions. \n",
      "However, neologisms in contemporary digital \n",
      "environments exhibit inherently un\n"
     ]
    }
   ],
   "source": [
    "for text in texts[:10]:\n",
    "    print(text.page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0925ec1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2b06717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    '''단계적으로 생각하세요. 당신은 NLP 전문가입니다. {docs}를 참고해서 전체적으로 어떤 내용인지 요약해주면 됩니다.\n",
    "    또한 핵심 기술을 설명해주면 됩니다.'''\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ddf37748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "  +--------------------+   \n",
      "  | ChatPromptTemplate |   \n",
      "  +--------------------+   \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +------------+       \n",
      "      | ChatOllama |       \n",
      "      +------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "print(chain.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a0daceca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 전체 요약\n",
      "\n",
      "이 문서는 **네오로그리즘(Neologism)을 탐지하기 위한 문맥 인지된 Named Entity Recognition (NER) 모델**에 대한 연구 논문입니다. 특히, 네오로그리즘을 정확하게 식별하는 데 중점을 두고 있으며, 이를 위해 딥러닝 기반의 모델을 활용합니다. 연구팀은 다양한 딥러닝 모델(LSTM-CRF, BERT 등)을 비교 분석하고, 문맥 정보를 효과적으로 활용하여 NER 성능을 향상시키는 방법을 제시합니다. 또한, 연구 결과는 네오로그리즘 탐지 분야에 기여할 수 있는 잠재력을 보여줍니다.\n",
      "\n",
      "## 핵심 기술\n",
      "\n",
      "*   **Named Entity Recognition (NER):**  텍스트에서 특정 유형의 개체(예: 사람, 장소, 조직 등)를 식별하는 기술입니다. 이 논문에서는 네오로그리즘을 포함한 개체를 식별하는 데 사용됩니다.\n",
      "*   **딥러닝 (Deep Learning):**  복잡한 패턴을 학습하기 위해 인공 신경망을 깊게 쌓아 올린 기술입니다. 이 논문에서는 LSTM-CRF, BERT와 같은 딥러닝 모델을 활용하여 NER 성능을 향상시킵니다.\n",
      "    *   **LSTM (Long Short-Term Memory):**  순환 신경망(RNN)의 한 종류로, 장기 의존성을 학습하는 데 효과적입니다.\n",
      "    *   **CRF (Conditional Random Field):**  시퀀스 레이블링 문제 해결에 사용되는 모델로, 시퀀스 내의 레이블 간의 의존성을 고려합니다.\n",
      "    *   **BERT (Bidirectional Encoder Representations from Transformers):**  Google에서 개발한 트랜스포머 기반의 사전 훈련된 언어 모델입니다. 문맥 정보를 효과적으로 활용하여 다양한 NLP 작업에서 뛰어난 성능을 보입니다.\n",
      "*   **문맥 인지 (Contextual Awareness):**  단어 자체의 의미뿐만 아니라, 주변 문맥을 고려하여 개체를 식별하는 기술입니다. 이 논문에서는 BERT와 같은 모델을 사용하여 문맥 정보를 활용하여 네오로그리즘 탐지 성능을 높입니다.\n",
      "\n",
      "**요약하자면, 이 논문은 딥러닝 기술, 특히 BERT와 같은 사전 훈련된 언어 모델을 활용하여 문맥 정보를 효과적으로 활용함으로써 네오로그리즘 탐지 성능을 향상시키는 방법을 제시합니다.**\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b5cb89",
   "metadata": {},
   "source": [
    "### Pydantic으로 모델의 응답을 구조화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c6174659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from datetime import date\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class PaperSummary(BaseModel):\n",
    "    title: str = Field(..., description=\"논문의 제목\")\n",
    "    authors: List[str] = Field(..., description=\"논문의 저자 목록\")\n",
    "    publication_date: Optional[str] = Field(None, description=\"논문이 출판된 날짜 또는 연도 (YYYY-MM-DD 또는 YYYY)\")\n",
    "    keywords: List[str] = Field(..., description=\"논문의 핵심 키워드 목록\")\n",
    "    abstract_summary: str = Field(..., description=\"논문 초록의 핵심 내용을 요약\")\n",
    "    main_contributions: List[str] = Field(..., description=\"논문이 기여한 주요 내용들을 목록으로 요약\")\n",
    "    methodology: str = Field(..., description=\"논문에서 사용된 주요 방법론을 간결하게 설명\")\n",
    "    results: str = Field(..., description=\"논문 실험의 주요 결과를 요약\")\n",
    "\n",
    "parser = StrOutputParser(pydantic_object=PaperSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "aceb8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Pydantic 모델의 JSON 스키마를 가져옵니다.\n",
    "json_schema = PaperSummary.model_json_schema()\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"당신은 논문을 분석하고 요약하는 전문가입니다.\n",
    "    사용자가 제공한 논문 내용을 분석하여, 아래에 제공된 JSON 스키마 형식에 맞춰서 응답을 생성해 주세요.\n",
    "\n",
    "    <논문 내용>\n",
    "    {paper_content}\n",
    "    </논문 내용>\n",
    "\n",
    "    <출력 JSON 스키마>\n",
    "    {json_schema}\n",
    "    </출력 JSON 스키마>\n",
    "\n",
    "    주의사항:\n",
    "    - 논문 내용에서 핵심 정보를 추출하여 JSON 객체를 완성하세요.\n",
    "    - JSON 형식으로만 응답해야 하며, 다른 부가적인 설명은 절대 포함하지 마세요.\n",
    "    - 모든 필드는 스키마에 정의된 데이터 타입과 일치해야 합니다.\n",
    "    \"\"\",\n",
    "    input_variables=[\"paper_content\"],\n",
    "    partial_variables={\"json_schema\": json_schema}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2dc32209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +------------+       \n",
      "      | ChatOllama |       \n",
      "      +------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "summary_chain = prompt | llm | parser \n",
    "print(summary_chain.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fe3498e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"논문 제목\": \"Context-Aware Named Entity Recognition for Neologisms\",\n",
      "  \"저자\": [\n",
      "    \"Erin Espriu\",\n",
      "    \"Junhaeng Lee\",\n",
      "    \"HyunYoung Oh\",\n",
      "    \"SangWoo Park\",\n",
      "    \"Young-myoung Kang\"\n",
      "  ],\n",
      "  \"주요 내용\": \"본 논문은 새로운 단어(neologisms)를 인식하는 데 초점을 맞춘 Named Entity Recognition (NER) 기술을 제안합니다.  특히, 언어 사용자의 세대 간 차이를 고려하여 NER 모델을 개선하는 방법을 제시합니다.  논문은 다양한 NER 모델(LSTM-CRF, BERT 등)을 활용하고,  세대별 언어 사용 패턴을 학습하여 모델 성능을 향상시키는 방법을 설명합니다.  또한,  NER 모델의 성능을 평가하기 위한 실험 결과를 제시합니다.\",\n",
      "  \"핵심 기술\": [\n",
      "    \"Named Entity Recognition (NER)\",\n",
      "    \"LSTM-CRF 모델\",\n",
      "    \"BERT 모델\",\n",
      "    \"세대별 언어 패턴 학습\"\n",
      "  ],\n",
      "  \"연구 배경\": \"새로운 단어(neologisms)는 언어 변화의 중요한 지표이며,  NER 모델이 이러한 변화를 정확하게 인식하는 것이 중요합니다.  특히,  세대 간 언어 사용 패턴의 차이를 고려하지 않으면 NER 모델의 성능이 저하될 수 있습니다.\"\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Trouble Shoothing이 발생하는데, 이는 LLM의 응답이 사전에 정의된 형식과 일치하지 않기 때문이다. \n",
    "print(summary_chain.invoke(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c3a3d",
   "metadata": {},
   "source": [
    "### Json 데이터 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3b3c3cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\\\n",
    "Can you create 100 city-based store locations for a fictitious company. The cities should be the kind of location where \\\n",
    "you might find a big box store like Target or Walmart. They should be located in the USA, or Europe, or Asia.\n",
    "\n",
    "Provide your response in a list. Only return the list. Do not make any additional comments.\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6f2f2fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = llm.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "977f566a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Atlanta, GA\\n2. Baltimore, MD\\n3. Boston, MA\\n4. Buffalo, NY\\n5. Charlotte, NC\\n6. Chicago, IL\\n7. Cincinnati, OH\\n8. Cleveland, OH\\n9. Columbus, OH\\n10. Colorado Springs, CO\\n11. Corpus Christi, TX\\n12. Dallas, TX\\n13. Denver, CO\\n14. Des Moines, IA\\n15. Detroit, MI\\n16. El Paso, TX\\n17. Fort Wayne, IN\\n18. Fresno, CA\\n19. Greensboro, NC\\n20. Houston, TX\\n21. Jacksonville, FL\\n22. Kansas City, MO\\n23. Las Vegas, NV\\n24. Louisville, KY\\n25. Memphis, TN\\n26. Milwaukee, WI\\n27. Minneapolis, MN\\n28. Nashville, TN\\n29. New Orleans, LA\\n30. Newark, NJ\\n31. Orlando, FL\\n32. Oklahoma City, OK\\n33. Oakland, CA\\n34. Omaha, NE\\n35. Orlando, FL\\n36. Philadelphia, PA\\n37. Phoenix, AZ\\n38. Pittsburgh, PA\\n39. Portland, OR\\n40. Raleigh, NC\\n41. Richmond, VA\\n42. Riverside, CA\\n43. Sacramento, CA\\n44. Saint Louis, MO\\n45. San Antonio, TX\\n46. San Diego, CA\\n47. San Francisco, CA\\n48. San Jose, CA\\n49. Santa Ana, CA\\n50. Seattle, WA\\n51. Shreveport, LA\\n52. Sioux Falls, SD\\n53. Stockton, CA\\n54. St. Paul, MN\\n55. Stamford, CT\\n56. Tampa, FL\\n57. Tucson, AZ\\n58. Tulsa, OK\\n59. Virginia Beach, VA\\n60. Worcester, MA\\n61. Berlin, Germany\\n62. Birmingham, UK\\n63. Bristol, UK\\n64. Cardiff, UK\\n65. Dublin, Ireland\\n66. Glasgow, UK\\n67. Leeds, UK\\n68. Manchester, UK\\n69. Newcastle, UK\\n70. Nottingham, UK\\n71. Oxford, UK\\n72. Plymouth, UK\\n73. Portsmouth, UK\\n74. Reading, UK\\n75. Sheffield, UK\\n76. Southampton, UK\\n77. Stoke-on-Trent, UK\\n78. Swindon, UK\\n79. York, UK\\n80. Milan, Italy\\n81. Rome, Italy\\n82. Naples, Italy\\n83. Florence, Italy\\n84. Venice, Italy\\n85. Madrid, Spain\\n86. Barcelona, Spain\\n87. Valencia, Spain\\n88. Seville, Spain\\n89. Lisbon, Portugal\\n90. Porto, Portugal\\n91. Warsaw, Poland\\n92. Krakow, Poland\\n93. Prague, Czech Republic\\n94. Budapest, Hungary\\n95. Vienna, Austria\\n96. Berlin, Germany\\n97. Munich, Germany\\n98. Frankfurt, Germany\\n99. Cologne, Germany\\n100. Hamburg, Germany'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a16923de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Atlanta, GA',\n",
       " '2. Baltimore, MD',\n",
       " '3. Boston, MA',\n",
       " '4. Buffalo, NY',\n",
       " '5. Charlotte, NC',\n",
       " '6. Chicago, IL',\n",
       " '7. Cincinnati, OH',\n",
       " '8. Cleveland, OH',\n",
       " '9. Columbus, OH',\n",
       " '10. Colorado Springs, CO',\n",
       " '11. Corpus Christi, TX',\n",
       " '12. Dallas, TX',\n",
       " '13. Denver, CO',\n",
       " '14. Des Moines, IA',\n",
       " '15. Detroit, MI',\n",
       " '16. El Paso, TX',\n",
       " '17. Fort Wayne, IN',\n",
       " '18. Fresno, CA',\n",
       " '19. Greensboro, NC',\n",
       " '20. Houston, TX',\n",
       " '21. Jacksonville, FL',\n",
       " '22. Kansas City, MO',\n",
       " '23. Las Vegas, NV',\n",
       " '24. Louisville, KY',\n",
       " '25. Memphis, TN',\n",
       " '26. Milwaukee, WI',\n",
       " '27. Minneapolis, MN',\n",
       " '28. Nashville, TN',\n",
       " '29. New Orleans, LA',\n",
       " '30. Newark, NJ',\n",
       " '31. Orlando, FL',\n",
       " '32. Oklahoma City, OK',\n",
       " '33. Oakland, CA',\n",
       " '34. Omaha, NE',\n",
       " '35. Orlando, FL',\n",
       " '36. Philadelphia, PA',\n",
       " '37. Phoenix, AZ',\n",
       " '38. Pittsburgh, PA',\n",
       " '39. Portland, OR',\n",
       " '40. Raleigh, NC',\n",
       " '41. Richmond, VA',\n",
       " '42. Riverside, CA',\n",
       " '43. Sacramento, CA',\n",
       " '44. Saint Louis, MO',\n",
       " '45. San Antonio, TX',\n",
       " '46. San Diego, CA',\n",
       " '47. San Francisco, CA',\n",
       " '48. San Jose, CA',\n",
       " '49. Santa Ana, CA',\n",
       " '50. Seattle, WA',\n",
       " '51. Shreveport, LA',\n",
       " '52. Sioux Falls, SD',\n",
       " '53. Stockton, CA',\n",
       " '54. St. Paul, MN',\n",
       " '55. Stamford, CT',\n",
       " '56. Tampa, FL',\n",
       " '57. Tucson, AZ',\n",
       " '58. Tulsa, OK',\n",
       " '59. Virginia Beach, VA',\n",
       " '60. Worcester, MA',\n",
       " '61. Berlin, Germany',\n",
       " '62. Birmingham, UK',\n",
       " '63. Bristol, UK',\n",
       " '64. Cardiff, UK',\n",
       " '65. Dublin, Ireland',\n",
       " '66. Glasgow, UK',\n",
       " '67. Leeds, UK',\n",
       " '68. Manchester, UK',\n",
       " '69. Newcastle, UK',\n",
       " '70. Nottingham, UK',\n",
       " '71. Oxford, UK',\n",
       " '72. Plymouth, UK',\n",
       " '73. Portsmouth, UK',\n",
       " '74. Reading, UK',\n",
       " '75. Sheffield, UK',\n",
       " '76. Southampton, UK',\n",
       " '77. Stoke-on-Trent, UK',\n",
       " '78. Swindon, UK',\n",
       " '79. York, UK',\n",
       " '80. Milan, Italy',\n",
       " '81. Rome, Italy',\n",
       " '82. Naples, Italy',\n",
       " '83. Florence, Italy',\n",
       " '84. Venice, Italy',\n",
       " '85. Madrid, Spain',\n",
       " '86. Barcelona, Spain',\n",
       " '87. Valencia, Spain',\n",
       " '88. Seville, Spain',\n",
       " '89. Lisbon, Portugal',\n",
       " '90. Porto, Portugal',\n",
       " '91. Warsaw, Poland',\n",
       " '92. Krakow, Poland',\n",
       " '93. Prague, Czech Republic',\n",
       " '94. Budapest, Hungary',\n",
       " '95. Vienna, Austria',\n",
       " '96. Berlin, Germany',\n",
       " '97. Munich, Germany',\n",
       " '98. Frankfurt, Germany',\n",
       " '99. Cologne, Germany',\n",
       " '100. Hamburg, Germany']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_locations = locations.strip().split(\"\\n\")\n",
    "raw_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9292b489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'location': 'Atlanta, GA'},\n",
       " {'location': 'Baltimore, MD'},\n",
       " {'location': 'Boston, MA'},\n",
       " {'location': 'Buffalo, NY'},\n",
       " {'location': 'Charlotte, NC'},\n",
       " {'location': 'Chicago, IL'},\n",
       " {'location': 'Cincinnati, OH'},\n",
       " {'location': 'Cleveland, OH'},\n",
       " {'location': 'Columbus, OH'},\n",
       " {'location': 'Colorado Springs, CO'},\n",
       " {'location': 'Corpus Christi, TX'},\n",
       " {'location': 'Dallas, TX'},\n",
       " {'location': 'Denver, CO'},\n",
       " {'location': 'Des Moines, IA'},\n",
       " {'location': 'Detroit, MI'},\n",
       " {'location': 'El Paso, TX'},\n",
       " {'location': 'Fort Wayne, IN'},\n",
       " {'location': 'Fresno, CA'},\n",
       " {'location': 'Greensboro, NC'},\n",
       " {'location': 'Houston, TX'},\n",
       " {'location': 'Jacksonville, FL'},\n",
       " {'location': 'Kansas City, MO'},\n",
       " {'location': 'Las Vegas, NV'},\n",
       " {'location': 'Louisville, KY'},\n",
       " {'location': 'Memphis, TN'},\n",
       " {'location': 'Milwaukee, WI'},\n",
       " {'location': 'Minneapolis, MN'},\n",
       " {'location': 'Nashville, TN'},\n",
       " {'location': 'New Orleans, LA'},\n",
       " {'location': 'Newark, NJ'},\n",
       " {'location': 'Orlando, FL'},\n",
       " {'location': 'Oklahoma City, OK'},\n",
       " {'location': 'Oakland, CA'},\n",
       " {'location': 'Omaha, NE'},\n",
       " {'location': 'Orlando, FL'},\n",
       " {'location': 'Philadelphia, PA'},\n",
       " {'location': 'Phoenix, AZ'},\n",
       " {'location': 'Pittsburgh, PA'},\n",
       " {'location': 'Portland, OR'},\n",
       " {'location': 'Raleigh, NC'},\n",
       " {'location': 'Richmond, VA'},\n",
       " {'location': 'Riverside, CA'},\n",
       " {'location': 'Sacramento, CA'},\n",
       " {'location': 'Saint Louis, MO'},\n",
       " {'location': 'San Antonio, TX'},\n",
       " {'location': 'San Diego, CA'},\n",
       " {'location': 'San Francisco, CA'},\n",
       " {'location': 'San Jose, CA'},\n",
       " {'location': 'Santa Ana, CA'},\n",
       " {'location': 'Seattle, WA'},\n",
       " {'location': 'Shreveport, LA'},\n",
       " {'location': 'Sioux Falls, SD'},\n",
       " {'location': 'Stockton, CA'},\n",
       " {'location': 'St'},\n",
       " {'location': 'Stamford, CT'},\n",
       " {'location': 'Tampa, FL'},\n",
       " {'location': 'Tucson, AZ'},\n",
       " {'location': 'Tulsa, OK'},\n",
       " {'location': 'Virginia Beach, VA'},\n",
       " {'location': 'Worcester, MA'},\n",
       " {'location': 'Berlin, Germany'},\n",
       " {'location': 'Birmingham, UK'},\n",
       " {'location': 'Bristol, UK'},\n",
       " {'location': 'Cardiff, UK'},\n",
       " {'location': 'Dublin, Ireland'},\n",
       " {'location': 'Glasgow, UK'},\n",
       " {'location': 'Leeds, UK'},\n",
       " {'location': 'Manchester, UK'},\n",
       " {'location': 'Newcastle, UK'},\n",
       " {'location': 'Nottingham, UK'},\n",
       " {'location': 'Oxford, UK'},\n",
       " {'location': 'Plymouth, UK'},\n",
       " {'location': 'Portsmouth, UK'},\n",
       " {'location': 'Reading, UK'},\n",
       " {'location': 'Sheffield, UK'},\n",
       " {'location': 'Southampton, UK'},\n",
       " {'location': 'Stoke-on-Trent, UK'},\n",
       " {'location': 'Swindon, UK'},\n",
       " {'location': 'York, UK'},\n",
       " {'location': 'Milan, Italy'},\n",
       " {'location': 'Rome, Italy'},\n",
       " {'location': 'Naples, Italy'},\n",
       " {'location': 'Florence, Italy'},\n",
       " {'location': 'Venice, Italy'},\n",
       " {'location': 'Madrid, Spain'},\n",
       " {'location': 'Barcelona, Spain'},\n",
       " {'location': 'Valencia, Spain'},\n",
       " {'location': 'Seville, Spain'},\n",
       " {'location': 'Lisbon, Portugal'},\n",
       " {'location': 'Porto, Portugal'},\n",
       " {'location': 'Warsaw, Poland'},\n",
       " {'location': 'Krakow, Poland'},\n",
       " {'location': 'Prague, Czech Republic'},\n",
       " {'location': 'Budapest, Hungary'},\n",
       " {'location': 'Vienna, Austria'},\n",
       " {'location': 'Berlin, Germany'},\n",
       " {'location': 'Munich, Germany'},\n",
       " {'location': 'Frankfurt, Germany'},\n",
       " {'location': 'Cologne, Germany'},\n",
       " {'location': 'Hamburg, Germany'}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_json = [{\"location\": location.split(\". \")[1]} for location in raw_locations]\n",
    "locations_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "30047044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os \n",
    "\n",
    "def write_jsonl(list_of_dict, file_name):\n",
    "    with open(file_name, \"w\") as file:\n",
    "        for dictionary in list_of_dict:\n",
    "            file.write(json.dumps(dictionary) + \"\\n\")\n",
    "            \n",
    "write_jsonl(locations_json, \"data/store_locations.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1514a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
