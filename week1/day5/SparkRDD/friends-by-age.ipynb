{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e33d3541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = \"C:/Users/user/anaconda3/envs/torchenv/python.exe\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"C:/Users/user/anaconda3/envs/torchenv/python.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ba3165",
   "metadata": {},
   "source": [
    "### Spark RDD\n",
    "- Spark 1.x, Spark 2.x에서의 전톧적인 RDD \n",
    "- `Sparkconf()`와 `SparkContext()`를 통합한 방법이 DataFrame으로 생성하는 `SparkSession()`\n",
    "\n",
    "### SparkContext\n",
    "- Spark 애플리케이션의 **entry point** 역할로, Spark 클러스터와 연결하고 RDD(Resilient Distributed Dataset)를 생성하거나 브로드캐스트 변수 등을 관리하며 Spark 작업 환경을 설정하고 제어하는 데 사용\n",
    "- `SparkContext` 자체는 데이터를 담고 있지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0451bf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-BC2FU6J:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FreiendsByAge</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=FreiendsByAge>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster('local').setAppName('FreiendsByAge')\n",
    "sc = SparkContext(conf = conf) \n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ccdce1",
   "metadata": {},
   "source": [
    "### 나이별 친구 수 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b71f108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "  fields = line.split(\",\")\n",
    "  age = int(fields[2])\n",
    "  num_friends = int(fields[3])\n",
    "  \n",
    "  return (age, num_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b520cd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:///nvidia_course/nvidia-course/week1/day5/SparkRDD/data/fakefriends.csv MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"file:///nvidia_course/nvidia-course/week1/day5/SparkRDD/data/fakefriends.csv\")\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0884ba51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = lines.map(parse_line)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594f6d0a",
   "metadata": {},
   "source": [
    "- `mapValues()`: RDD의 값(value) 부분만 변환\n",
    "- `reduceByKey()`: 동일한 키를 가진 값들을 그룹화하여 람다 함수를 이용해 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36cdf440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value 값만 변환하므로 만약 데이터 (33, 385)이라면, (33, (385 , 1))로 반환\n",
    "# 33세의 평균 친구의 수를 구하기 위해 몇 명의 33세가 있는지도 고려해야함\n",
    "# reduceByKey를 활용하여 각 33세의 친구 수와 33세의 등장 횟수를 각각 키를 기준으로 계산\n",
    "totals_by_age = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1489e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_by_age = totals_by_age.mapValues(lambda x: x[0] / x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee64768f",
   "metadata": {},
   "source": [
    "### DAG로 Spark가 최적의 방법을 찾아서 작동\n",
    "- `collect()`: PySpark에서 액션(Action)변환으로, DAG(Direted Acyclic Graph)와 지연 평가(Lazy Evaluation)라는 Spark와 관련이 있음\n",
    "  - RDD 또는 DataFrame의 모든 데이터를 Spark 클러스터의 드라이버 프로그램으로 가져오는 액션으로, 분산되어 있는 데이터를 한 곳으로 모으는 역할\n",
    "  - OOM(Out of Memory) 문제를 일으키거나 네트워크 부하 가능성 O\n",
    "\n",
    "- `지연 평가(lazy Evaluation)`\n",
    "  - Spark의 모든 변환(Transformation)(ex. map, filter, reduceByKey)은 모두 지연적으로 평가\n",
    "  - 변환 코드를 작성해서 Spark는 즉시 연산을 수행하지 않고 실행 계획만 세워둔 후, 실제 연산은 액션(Action)(ex. collect(), count(), show(), write)이 호출될 때 실행됨\n",
    "\n",
    "- `DAG(Directed Acyclic Graph)`: Spark는 지연 평가 방식을 통해 DAG 실행 모델 구축\n",
    "  - Directed(방향성): 작업 흐름이 한 방향으로만 진행되어 되돌아가거나 순환 X \n",
    "  - Acyclic(비순환): 작업 그래프에 순환 X\n",
    "  - Graph(그래프): 일련의 노드(Spark, DataFrame)와 엣지(Transformation)로 구성된 집합\n",
    "\n",
    "`collect()`가 호출되면:\n",
    "\n",
    "1. Spark는 collect()까지의 전체 변환 체인을 분석하여 DAG를 생성\n",
    "\n",
    "2. DAG 스케줄러는 이 DAG를 기반으로 스테이지(Stage) 를 나누는데, 스테이지는 셔플(shuffle) 같은 넓은 변환(Wide Transformation)을 기준으로 나뉨 \n",
    "\n",
    "3. 각 스테이지 내에서 태스크(Task) 들이 생성되고, 이 태스크들이 클러스터의 워커 노드에서 병렬로 실행\n",
    "\n",
    "4. 모든 태스크가 완료되면, collect()는 최종 결과를 드라이버 프로그램으로 다시 모음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f77ae562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 325.3333333333333)\n",
      "(26, 242.05882352941177)\n",
      "(55, 295.53846153846155)\n",
      "(40, 250.8235294117647)\n",
      "(68, 269.6)\n",
      "(59, 220.0)\n",
      "(37, 249.33333333333334)\n",
      "(54, 278.0769230769231)\n",
      "(38, 193.53333333333333)\n",
      "(27, 228.125)\n",
      "(53, 222.85714285714286)\n",
      "(57, 258.8333333333333)\n",
      "(56, 306.6666666666667)\n",
      "(43, 230.57142857142858)\n",
      "(36, 246.6)\n",
      "(22, 206.42857142857142)\n",
      "(35, 211.625)\n",
      "(45, 309.53846153846155)\n",
      "(60, 202.71428571428572)\n",
      "(67, 214.625)\n",
      "(19, 213.27272727272728)\n",
      "(30, 235.8181818181818)\n",
      "(51, 302.14285714285717)\n",
      "(25, 197.45454545454547)\n",
      "(21, 350.875)\n",
      "(42, 303.5)\n",
      "(49, 184.66666666666666)\n",
      "(48, 281.4)\n",
      "(50, 254.6)\n",
      "(39, 169.28571428571428)\n",
      "(32, 207.9090909090909)\n",
      "(58, 116.54545454545455)\n",
      "(64, 281.3333333333333)\n",
      "(31, 267.25)\n",
      "(52, 340.6363636363636)\n",
      "(24, 233.8)\n",
      "(20, 165.0)\n",
      "(62, 220.76923076923077)\n",
      "(41, 268.55555555555554)\n",
      "(44, 282.1666666666667)\n",
      "(69, 235.2)\n",
      "(65, 298.2)\n",
      "(61, 256.22222222222223)\n",
      "(28, 209.1)\n",
      "(66, 276.44444444444446)\n",
      "(46, 223.69230769230768)\n",
      "(29, 215.91666666666666)\n",
      "(18, 343.375)\n",
      "(47, 233.22222222222223)\n",
      "(34, 245.5)\n",
      "(63, 384.0)\n",
      "(23, 246.3)\n"
     ]
    }
   ],
   "source": [
    "results = avg_by_age.collect()\n",
    "for res in results:\n",
    "  print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
