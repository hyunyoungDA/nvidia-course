{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eef34b3",
   "metadata": {},
   "source": [
    "### 기본 RNN 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ebf6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddedcebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"simple rnn\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"simple rnn\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m520\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m210\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">730</span> (2.85 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m730\u001b[0m (2.85 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">730</span> (2.85 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m730\u001b[0m (2.85 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential(name = \"simple rnn\")\n",
    "\n",
    "model.add(Input(shape = (10,5)))\n",
    "model.add(SimpleRNN(20))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec10196f",
   "metadata": {},
   "source": [
    "### RNN기반 시계열 데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83f76094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import RNN, Dense, Input, SimpleRNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbab7944",
   "metadata": {},
   "source": [
    "### 임의 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0581469e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 분류 데이터 생성 시작 ---\n",
      "생성된 DataFrame 형태: (1000, 3)\n",
      "X (특징) 형태: (1000, 2)\n",
      "y (타겟 클래스) 형태: (1000,)\n",
      "\n",
      "--- 생성된 데이터의 처음 5개 행: ---\n",
      "   feature_0  feature_1  target\n",
      "0   0.601034   1.535353       1\n",
      "1   0.755945  -1.172352       0\n",
      "2   1.354479  -0.948528       0\n",
      "3   3.103090   0.233485       0\n",
      "4   0.753178   0.787514       1\n"
     ]
    }
   ],
   "source": [
    "def create_classification_data(n_samples=1000, n_features=2, n_informative=2,\n",
    "                               n_redundant=0, n_clusters_per_class=1, n_classes=2, random_state=42):\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=n_informative,\n",
    "        n_redundant=n_redundant,\n",
    "        n_clusters_per_class=n_clusters_per_class,\n",
    "        n_classes=n_classes,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    # DataFrame으로 변환하여 보기 쉽게 만듭니다.\n",
    "    # 컬럼 이름은 'feature_0', 'feature_1', ..., 'target'\n",
    "    feature_names = [f'feature_{i}' for i in range(n_features)]\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df['target'] = y\n",
    "    return df, X, y\n",
    "\n",
    "print(\"--- 분류 데이터 생성 시작 ---\")\n",
    "# 2개의 특징을 가진 이진 분류 데이터 생성 (2개의 클래스)\n",
    "# 각 클래스 내에 1개의 클러스터가 있어 비교적 선형적으로 분류 가능합니다.\n",
    "df_classification, X_classification, y_classification = create_classification_data(\n",
    "    n_samples=1000,       # 1000개의 데이터 포인트\n",
    "    n_features=2,         # 2개의 특징 (x, y 좌표처럼 시각화하기 좋음)\n",
    "    n_informative=2,      # 2개의 특징 모두 분류에 유용함\n",
    "    n_redundant=0,        # 중복 특징 없음\n",
    "    n_clusters_per_class=1, # 각 클래스는 하나의 명확한 군집을 형성\n",
    "    n_classes=2,          # 2개의 클래스 (0 또는 1)\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"생성된 DataFrame 형태: {df_classification.shape}\")\n",
    "print(f\"X (특징) 형태: {X_classification.shape}\")\n",
    "print(f\"y (타겟 클래스) 형태: {y_classification.shape}\")\n",
    "print(\"\\n--- 생성된 데이터의 처음 5개 행: ---\")\n",
    "print(df_classification.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ba3b299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 스케일링 결과 (처음 5개 행) ---\n",
      "X_train_scaled_cls:\n",
      " [[ 0.42103266  0.43750578]\n",
      " [-0.30480465 -0.86556712]\n",
      " [ 0.68552745  0.16742658]\n",
      " [-2.21033447 -1.8230404 ]\n",
      " [ 0.58482223 -0.47622049]]\n",
      "X_test_scaled_cls:\n",
      " [[ 0.16785555  0.43832313]\n",
      " [-1.6602748  -1.59042596]\n",
      " [-0.04814766 -0.43506505]\n",
      " [-0.65959026 -1.33810021]\n",
      " [-1.99622754 -1.55888685]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_classification, y_classification, test_size = 0.2, random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled_cls = scaler.fit_transform(X_train)\n",
    "X_test_scaled_cls = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n--- 스케일링 결과 (처음 5개 행) ---\")\n",
    "print(\"X_train_scaled_cls:\\n\", X_train_scaled_cls[:5])\n",
    "print(\"X_test_scaled_cls:\\n\", X_test_scaled_cls[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2403c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = X_train_scaled_cls.reshape(X_train_scaled_cls.shape[0], X_train_scaled_cls.shape[1], 1)\n",
    "X_test_final = X_test_scaled_cls.reshape(X_test_scaled_cls.shape[0], X_test_scaled_cls.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64987744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 2, 1), (200, 2, 1))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape, X_test_final.shape # 데이터 개수, 시퀀스 길이, 특징 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "006e11d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 50\n",
    "num_sequences = 1000\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape = (sequence_length, 1)))\n",
    "# 현재 SimpleRNN 층을 다음 RNN 층에 전달\n",
    "model.add(SimpleRNN(units = 64, activation = 'relu', return_sequences = True))\n",
    "model.add(SimpleRNN(units = 128, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9468202",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.optimizers.Adam(learning_rate=1e-4),\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fba95f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 모델 훈련 시작 ---\n",
      "Epoch 1/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9234 - loss: 0.1652 - val_accuracy: 0.9500 - val_loss: 0.1496\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9499 - loss: 0.1258 - val_accuracy: 0.9375 - val_loss: 0.1477\n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9454 - loss: 0.1326 - val_accuracy: 0.9375 - val_loss: 0.1505\n",
      "Epoch 4/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9446 - loss: 0.1360 - val_accuracy: 0.9375 - val_loss: 0.1511\n",
      "Epoch 5/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9313 - loss: 0.1755 - val_accuracy: 0.9375 - val_loss: 0.1504\n",
      "Epoch 6/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9288 - loss: 0.1603 - val_accuracy: 0.9375 - val_loss: 0.1484\n",
      "Epoch 7/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9370 - loss: 0.1404 - val_accuracy: 0.9250 - val_loss: 0.1487\n",
      "Epoch 8/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9382 - loss: 0.1425 - val_accuracy: 0.9375 - val_loss: 0.1502\n",
      "Epoch 9/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9354 - loss: 0.1553 - val_accuracy: 0.9375 - val_loss: 0.1472\n",
      "Epoch 10/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9419 - loss: 0.1569 - val_accuracy: 0.9250 - val_loss: 0.1479\n",
      "Epoch 11/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9369 - loss: 0.1464 - val_accuracy: 0.9375 - val_loss: 0.1512\n",
      "Epoch 12/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9369 - loss: 0.1415 - val_accuracy: 0.9375 - val_loss: 0.1497\n",
      "Epoch 13/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9412 - loss: 0.1438 - val_accuracy: 0.9375 - val_loss: 0.1467\n",
      "Epoch 14/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9289 - loss: 0.1502 - val_accuracy: 0.9500 - val_loss: 0.1467\n",
      "Epoch 15/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9435 - loss: 0.1288 - val_accuracy: 0.9375 - val_loss: 0.1512\n",
      "Epoch 16/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9472 - loss: 0.1261 - val_accuracy: 0.9250 - val_loss: 0.1480\n",
      "Epoch 17/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9315 - loss: 0.1564 - val_accuracy: 0.9375 - val_loss: 0.1481\n",
      "Epoch 18/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9407 - loss: 0.1435 - val_accuracy: 0.9500 - val_loss: 0.1485\n",
      "Epoch 19/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9428 - loss: 0.1353 - val_accuracy: 0.9375 - val_loss: 0.1497\n",
      "Epoch 20/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9430 - loss: 0.1367 - val_accuracy: 0.9375 - val_loss: 0.1494\n",
      "Epoch 21/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9277 - loss: 0.1489 - val_accuracy: 0.9375 - val_loss: 0.1480\n",
      "Epoch 22/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9275 - loss: 0.1508 - val_accuracy: 0.9375 - val_loss: 0.1459\n",
      "Epoch 23/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9439 - loss: 0.1362 - val_accuracy: 0.9375 - val_loss: 0.1497\n",
      "Epoch 24/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9314 - loss: 0.1414 - val_accuracy: 0.9375 - val_loss: 0.1501\n",
      "Epoch 25/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9359 - loss: 0.1430 - val_accuracy: 0.9375 - val_loss: 0.1505\n",
      "Epoch 26/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9307 - loss: 0.1458 - val_accuracy: 0.9375 - val_loss: 0.1498\n",
      "Epoch 27/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9394 - loss: 0.1465 - val_accuracy: 0.9375 - val_loss: 0.1474\n",
      "Epoch 28/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9315 - loss: 0.1490 - val_accuracy: 0.9250 - val_loss: 0.1487\n",
      "Epoch 29/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9468 - loss: 0.1175 - val_accuracy: 0.9500 - val_loss: 0.1523\n",
      "Epoch 30/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9411 - loss: 0.1372 - val_accuracy: 0.9375 - val_loss: 0.1491\n",
      "Epoch 31/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9493 - loss: 0.1228 - val_accuracy: 0.9500 - val_loss: 0.1488\n",
      "Epoch 32/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9375 - loss: 0.1392 - val_accuracy: 0.9375 - val_loss: 0.1472\n",
      "--- 모델 훈련 완료 ---\n"
     ]
    }
   ],
   "source": [
    "callbacks = EarlyStopping(monitor = 'val_loss', patience = 10, restore_best_weights=True)\n",
    "\n",
    "# 모델 훈련\n",
    "print(\"\\n--- 모델 훈련 시작 ---\")\n",
    "history = model.fit(\n",
    "    X_train_final, y_train,\n",
    "    epochs=50,             # 최대 에포크 수\n",
    "    batch_size=32,          # 배치 크기\n",
    "    validation_split=0.1,   # 훈련 데이터의 10%를 검증 데이터로 사용\n",
    "    callbacks=callbacks, # EarlyStopping 콜백 적용\n",
    "    verbose=1               # 훈련 진행 상황 표시\n",
    ")\n",
    "print(\"--- 모델 훈련 완료 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca1ce0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.198 | acc: 0.915\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "loss,acc = model.evaluate(X_test_final, y_test, verbose=0)\n",
    "print(f\"loss: {loss:.3f} | acc: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a8d0c",
   "metadata": {},
   "source": [
    "### LSTM 모델을 활용한 자연어 감정 분류 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61a3d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, TextVectorization, Embedding, LSTM\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c20c62",
   "metadata": {},
   "source": [
    "### Tensorflow Dataset 구성 (10점)\n",
    "- `keras.utils.text_dataset_from_directory()`를 활용하여 저장된 텍스트 파일로부터 tensorflow Dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a037fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nraw_train_dataset = tf.keras.preprocessing.text_dataset_from_directory(\\n    train_data_path,\\n    labels='inferred',\\n    label_mode='binary',\\n    batch_size=batch_size,\\n    class_names=['neg', 'pos'],\\n    shuffle=True,\\n    seed=42,\\n    validation_split=0.2,\\n    subset='training'\\n)\\n\\nraw_val_dataset = tf.keras.preprocessing.text_dataset_from_directory(\\n    train_data_path,\\n    labels='inferred',\\n    label_mode='binary',\\n    batch_size=batch_size,\\n    class_names=['neg', 'pos'],\\n    shuffle=True,\\n    seed=42,\\n    validation_split=0.2,\\n    subset='validation'\\n)\\n\\nraw_test_dataset = tf.keras.preprocessing.text_dataset_from_directory(\\n    test_data_path,\\n    labels=None,\\n    label_mode=None,\\n    batch_size=batch_size,\\n    shuffle=False,\\n)\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = \"/mnt/elice/dataset/train\"\n",
    "test_data_path = \"/mnt/elice/dataset/test\"\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "raw_train_dataset = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    train_data_path,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    batch_size=batch_size,\n",
    "    class_names=['neg', 'pos'],\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.2,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "raw_val_dataset = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    train_data_path,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    batch_size=batch_size,\n",
    "    class_names=['neg', 'pos'],\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.2,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "raw_test_dataset = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    test_data_path,\n",
    "    labels=None,\n",
    "    label_mode=None,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc74401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "# 현재 imdb 데이터셋을 keras에서 지원해주므로 로드\n",
    "# from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# max_words = 500 # 사전에 사용할 최대 단어 수, 상위 500개만 사용함 \n",
    "\n",
    "# (train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=max_words)\n",
    "# print(train_input.shape, test_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ec71ad",
   "metadata": {},
   "source": [
    "### 데이터 전처리 (10점)\n",
    "- 텍스트 소문자 변환\n",
    "- 불용어(stopword)제거\n",
    "  - 여기서는 문장부호 및 **br** 태그만 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5d56f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def text_standrization(input_text):\n",
    "  input_text_lowercase = tf.strings.lower(input_text)\n",
    "  input_text_processed = tf.strings.regex_replace(input_text_lowercase, \"<br />\",\" \")\n",
    "  punctuation_regex = f\"[{re.escape(string.punctuation)}]\" # 불용어 정의\n",
    "  \n",
    "  input_text_processed = tf.strings.regex_replace(input_text_processed, punctuation_regex, \"\")\n",
    "  return input_text_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dddb18",
   "metadata": {},
   "source": [
    "### 텍스트 데이터 인코딩 (20점)\n",
    "\n",
    "전처리 완료한 데이터를 정해진 규칙에 따라 숫자의 배열 (벡터)로 변환합니다.\n",
    "\n",
    "Keras의 `TextVectorization` 레이어를 활용해 가장 자주 등장하는 단어 순으로 숫자를 부여하고, 벡터로 변환하세요.\n",
    "\n",
    "- TextVectorization 레이어의 `standardize` 인자에는 전처리 함수인 text_standrization을 사용합니다.\n",
    "- max_tokens 인자에는 `max_words` 변수의 값을 사용합니다.\n",
    "- output_sequence_length 인자에는 `max_len` 변수의 값을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478a6c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_words: 훈련 데이터에서 가장 자주 등장하는 상위 max_words 개의 단어만 어휘 사전에 포함시키고, 나머지는 Unknown로 처리하는 단어의 개수\n",
    "# embedding_dim: 각 단어를 표현하는 벡터의 차원\n",
    "# max_len: 각 입력 시퀀스의 최대 단어 수\n",
    "\n",
    "max_words = 500\n",
    "embedding_dim = 150\n",
    "max_len = 256\n",
    "\n",
    "text_vectorization_layer = TextVectorization(\n",
    "  standardize = text_standrization,\n",
    "  max_tokens = max_words,\n",
    "  output_sequence_length = max_len,\n",
    ")\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return text_vectorization_layer(text), label\n",
    "\n",
    "def vectorize_text_test(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return text_vectorization_layer(text)\n",
    "  \n",
    "# 각 데이터셋에 vectorize_text 매핑\n",
    "train_dataset = raw_train_dataset.map(vectorize_text)\n",
    "val_dataset = raw_val_dataset.map(vectorize_text)\n",
    "test_dataset = raw_test_dataset.map(vectorize_text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d277431",
   "metadata": {},
   "source": [
    "### 모델 정의 및 컴파일 (20점)\n",
    "\n",
    "- A. Embedding Layer\n",
    "  - 벡터화된 자연어 데이터를 128차원의 벡터로 변환하는 Embedding Layer를 사용합니다.\n",
    "\n",
    "- B. LSTM Classifier\n",
    "  - 단일 LSTM Layer를 사용하며, 은닉층의 차원은 32입니다.\n",
    "  - 출력층의 차원은 1입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d84487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(None,)))\n",
    "# [지시사항 4] LSTM 모델을 구성하세요.\n",
    "# Hint. model.add()를 3번 사용합니다.\n",
    "model.add(Embedding(max_words, 128))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b50937",
   "metadata": {},
   "source": [
    "### 모델 학습 (20점)\n",
    "\n",
    "- epochs는 30으로 설정합니다.\n",
    "- 학습 데이터로는 train_dataset을 사용합니다.\n",
    "- 검증 데이터로는 val_dataset을 사용합니다.\n",
    "- 미리 정의된 callbacks 배열을 콜백 함수로 등록합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa61b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53afc006",
   "metadata": {},
   "source": [
    "### 모델 평가 (20점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7010fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "hist = model.fit(\n",
    "    X_train, y_train, epochs = 30, batch_size = 32, callbacks = callbacks, validation_data = (val_train, val_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kerasenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
