{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a657acc",
   "metadata": {},
   "source": [
    "## vLLM(Virvual large Language Model) HuggingFace ëª¨ë¸ ì„œë²„ë€?\n",
    "\n",
    "vLLMì€ **HuggingFace ê¸°ë°˜ì˜ LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)** ì„ **ë¹ ë¥´ê²Œ ì„œë¹™(ì„œë¹„ìŠ¤ ì œê³µ)** í•˜ê³  **Inference** í•  ìˆ˜ ìˆê²Œ ë„ì™€ì£¼ëŠ” ì˜¤í”ˆì†ŒìŠ¤ì´ë‹¤. ë¹„ë™ê¸°, ë³‘ë ¬ì ì¸ ìš”ì²­ ì²˜ë¦¬ì™€ KV ìºì‹œ ì¬ì‚¬ìš©ì— ì´ˆì ì„ ë§ì¶˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤.\n",
    "\n",
    "- PagedAttention: í•„ìš”í•œ ë¶€ë¶„ë§Œ ë¶€ë¶„ì ìœ¼ë¡œ KV cacheë¥¼ ê³µìœ í•˜ê³  ë¡œë”©í•´ì„œ VRAM ì ˆì•½\n",
    "- ë¹„ë™ê¸° ì¶”ë¡ : ë‹¤ìˆ˜ì˜ ìš”ì²­ì„ ë¹ ë¥´ê²Œ ì²˜ë¦¬ \n",
    "- OpenAI API í˜¸í™˜ ì„œë²„: í´ë¼ì´ì–¸íŠ¸ëŠ” `openai.ChatCompletion.create()`ì²˜ëŸ¼ ìš”ì²­ ê°€ëŠ¥ \n",
    "\n",
    " Linux OSë§Œ ì§€ì›í•˜ê¸° ë•Œë¬¸ì— vLLM ì‹¤ìŠµì€ Ubuntuì—ì„œ ì‹¤í–‰\n",
    "\n",
    "### vLLM ì„œë²„ê°€ í•˜ëŠ” ì¼\n",
    "- ëª¨ë¸ ë¡œë”©\n",
    "    - HuggingFaceì—ì„œ ì›í•˜ëŠ” ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê±°ë‚˜, ì´ë¯¸ ìºì‹œëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "\n",
    "- ëª¨ë¸ ìµœì í™”\n",
    "    - ì˜ˆ: ëª¨ë¸ì„ float16ì²˜ëŸ¼ ë” ê°€ë²¼ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì„œ ë” ë¹ ë¥´ê²Œ ì‹¤í–‰ë˜ë„ë¡ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "- íŒŒì´í”„ë¼ì¸ êµ¬ì„±\n",
    "    - ëª¨ë¸ì„ ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë¡œë“œí•©ë‹ˆë‹¤. (ì¶”ë¡ ì´ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì„¸íŒ…)\n",
    "\n",
    "- ì„±ëŠ¥ ìµœì í™”\n",
    "    - íŒŒì´í”„ë¼ì¸ ì¤‘ ì¼ë¶€ë¥¼ ë” ë¹ ë¥¸ ì—°ì‚° ë°©ì‹(ì˜ˆ: fused ops)ìœ¼ë¡œ ë°”ê¿” ì„±ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤.\n",
    "\n",
    "- API ì„œë²„ë¡œ ì œê³µ\n",
    "    - ëª¨ë¸ì— ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” í‘œì¤€í™”ëœ HTTP API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì—´ì–´ë‘¡ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9a7837",
   "metadata": {},
   "source": [
    "### vLLL environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e8b193",
   "metadata": {},
   "source": [
    "ê°€ìƒí™˜ê²½ ì‹¤í–‰\n",
    "```bash\n",
    "source ./vllm-env/bin/activate```\n",
    "\n",
    "PyTorch ì„¤ì¹˜ (CUDA 11.8 ê¸°ì¤€)\n",
    "```bash\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "\n",
    "vLLM ì„¤ì¹˜\n",
    "```bash\n",
    "pip install vllm\n",
    "```\n",
    "\n",
    "Optional: transformersì™€ accelerateë„ ì„¤ì¹˜\n",
    "```bash\n",
    "pip install transformers accelerate\n",
    "```\n",
    "\n",
    "---\n",
    "vllm ì„œë²„ ì‹¤í–‰\n",
    "```bash\n",
    "python3 -m vllm.entrypoints.openai.api_server \\\n",
    "    --model TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a04ed3",
   "metadata": {},
   "source": [
    "## vLLM ì„œë²„ ì„¤ì • \n",
    "\n",
    "### vLLM ì‹¤í–‰ ì˜ˆì‹œ\n",
    "```bash\n",
    "    python3 -m vllm.entrypoints.openai.api_server \\\n",
    "    --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 \\\n",
    "    --gpu-memory-utilization 0.5 \\\n",
    "    --dtype float16 \\\n",
    "    --port 8000\n",
    "\n",
    "```\n",
    "### NVIDIA Chat ì‚¬ìš©ì‹œ NVIDIAì—ì„œ ì§€ì›í•˜ëŠ” vLLM server í™•ì¸ í›„ ì‹¤í–‰ \n",
    "```bash\n",
    "    vllm serve microsoft/phi-3.5-vision-instruct  \\\n",
    "        --trust-remote-code \\\n",
    "        --max_model_len 16384 \\\n",
    "        --gpu-memory-utilization 0.8 \\\n",
    "        --enforce-eager \\\n",
    "        --port 9000\n",
    "```\n",
    "\n",
    "### ê° íŒŒë¼ë¯¸í„° ì„¤ëª… \n",
    "\n",
    "- `microsoft/phi-3.5-vision-instruct`: HuggingFaceì— ìˆëŠ” nulti-model ëª¨ë¸ ì´ë¦„ ì •ì˜ \n",
    "- `--trust-remote-code`: ëª¨ë¸ì˜ ì»¤ìŠ¤í…€ ì½”ë“œë¥¼ ì‹ ë¢°í•˜ê² ë‹¤ëŠ” ì˜ë¯¸, Vision ëª¨ë¸ì²˜ëŸ¼ ì½”ë“œê°€ í¬í•¨ëœ ëª¨ë¸ì—ì„œëŠ” í•„ìˆ˜\n",
    "- `--max-model-len`: ìµœëŒ€ context lengthë¥¼ ì§€ì • \n",
    "- `--gpu-memory-utillization`: GPU ë©”ëª¨ë¦¬ ì‚¬ìš© ë¹„ìœ¨ì„ ì„¤ì • \n",
    "- `--enforce-eager`: ì¼ë¶€ ëª¨ë¸ì—ì„œ LazyTensor ëŒ€ì‹  eager ëª¨ë“  ê°•ì œ(ë””ë²„ê¹…, í˜¸í™˜ì„± ëª©ì )\n",
    "- `--port 9000`: API serverê°€ ì—´ë¦¬ëŠ” í¬íŠ¸ ì§€ì • "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169aea5d",
   "metadata": {},
   "source": [
    "### vLLM serverì˜ LLM í˜¸ì¶œ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93b1c5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffusion models are statistical algorithms that try to infer the underlying properties of a network based on the indirect influences of nodes on each other. The concept of diffusion models is based on the concept of nearest neighbors, which means that the influence of one node on another node can be strengthened or weakened by the influence of other nodes that are closer to both nodes.\n",
      "\n",
      "By assuming that nodes have mutual influence on each other, the diffusion model can be used to infer the properties of the network. These properties can include the degree distribution, clustering coefficient, community structure, and centrality measures.\n",
      "\n",
      "The most common diffusion models used in network analysis include the following:\n",
      "\n",
      "1. Greedy Algorithm: This model assumes that the network is initially well-connected. The algorithm iterates through the network, adding new nodes and edges, until the network is fully connected.\n",
      "\n",
      "2. Random Walk with Restart (RWAR) Algorithm: This model assumes that the network is initially well-connected, but the network is interconnected in a random way. The algorithm iterates through the network, adding new nodes and edges, until the network is fully connected.\n",
      "\n",
      "3. Small World Model: This model describes the network as a power-law-distributed network, where nodes are randomly connected but the network is tightly clustered at the core.\n",
      "\n",
      "4. Degree Distribution: This model assumes that the network is initially well-connected, but the degree distribution of the network is non-linear. It gradually narrows as the network becomes denser, resulting in a power-law distribution of node degrees.\n",
      "\n",
      "5. Community Detection: This model detects the structure of the network as communities, where nodes are grouped based on their similarity or interaction patterns.\n",
      "\n",
      "In summary, diffusion models are a powerful tool for inferring the underlying properties of a network based on the indirect influences of nodes on each other. They can provide insight into network structure, connectivity, and dynamics.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os \n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",  # vLLM ì„œë²„ ì£¼ì†Œ\n",
    "    api_key=\"EMPTY\"  # í‚¤ ì—†ì–´ë„ ë˜ì§€ë§Œ ë°˜ë“œì‹œ ë¬¸ìì—´ í•„ìš”í•¨\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = model_name,  # ì„œë²„ì—ì„œ ë¡œë”©í•œ ëª¨ë¸ ì´ë¦„\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain the concept of diffusion models.\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af11b998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Completion(id='cmpl-c475f686f60a457ea0428a745f22264f', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text=\" I hope you are well. I think I'm missing you. You have\", stop_reason=None, prompt_logprobs=None)], created=1754026683, model='TinyLlama/TinyLlama-1.1B-Chat-v1.0', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=16, prompt_tokens=9, total_tokens=25, completion_tokens_details=None, prompt_tokens_details=None), service_tier=None, kv_transfer_params=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.completions.create(\n",
    "    model = model_name,\n",
    "    prompt = \"Hello! How's it going?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0932b7",
   "metadata": {},
   "source": [
    "### vLLMê³¼ LangChain ì—°ë™ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7747f98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11636\\4140960587.py:11: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(llm.predict(\"What's the capital of France?\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url = \"http://localhost:8000/v1\",\n",
    "    api_key = 'EMPTY',\n",
    "    model_name = model_name,\n",
    "    temperature = 0.7\n",
    ")\n",
    "\n",
    "print(llm.predict(\"What's the capital of France?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d61612",
   "metadata": {},
   "source": [
    "### vLLMê³¼ LangChain ê¸°ë°˜ ê°„ë‹¨í•œ Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a79bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11636\\2703758605.py:16: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: The 2022 FIFA World Cup, the most prestigious annual international football tournament, was won by Argentina.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url = \"http://localhost:8000/v1\",\n",
    "    api_key = 'EMPTY',\n",
    "    model_name = model_name,\n",
    "    temperature = 0.7\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content = \"You are a helpful assistant.\"),\n",
    "    HumanMessage(content = \"Hello, who won the World Cup in 2022?\")\n",
    "]\n",
    "\n",
    "response = llm(messages)\n",
    "print(f\"Bot: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b049e0",
   "metadata": {},
   "source": [
    "### Full-context\n",
    "\n",
    "- ëŒ€í™” ë©”ì‹œì§€ë¥¼ ê³„ì† ìŒ“ì•„ì„œ ì „ì²´ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ì „ë‹¬ \n",
    "- context ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ëŠë ¤ì§€ê³ , ë¹„ìš©ì´ ì¦ê°€ \n",
    "\n",
    "### Running State \n",
    "\n",
    "- ëŒ€í™” ì¤‘ ìƒíƒœë¥¼ ìœ ì§€í•˜ëŠ” ê²ƒìœ¼ë¡œ, ìƒíƒœ ë³€ìˆ˜ë¥¼ ë³„ë„ë¡œ ë§Œë“¤ì–´ì„œ ê´€ë¦¬í•œë‹¤. \n",
    "- ëŒ€í™” ì´ë ¥ ì¤‘ ì¤‘ìš”í•œ ë¶€ë¶„ë§Œ ë¶€ë¶„ì ìœ¼ë¡œ ìœ ì§€í•˜ê³  ìš”ì•½í•´ì„œ ê´€ë¦¬ ê°€ëŠ¥í•˜ë‹¤.\n",
    "- `langgraph`ë¡œ memory ì²˜ë¦¬ \n",
    "\n",
    "### Sliding Window\n",
    "\n",
    "- ìµœê·¼ Nê°œì˜ ë©”ì‹œì§€ë§Œ í”„ë¡¬í”„íŠ¸ë¡œ ì „ë‹¬\n",
    "- í† í° ê¸¸ì´ ì œí•œì— íš¨ê³¼ì \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54386d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boat: I don't have access to real-time weather updates, but according to current weather conditions, the weather today in your area is usually sunny and mild. The temperature may vary depending on elevation, but in general, the temperature is around 60Â°F (16Â°C) to 70Â°F (21Â°C). The wind direction is usually northerly or northeasterly, with gusts of up to 15 mph (24 km/h).\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url = \"http://localhost:8000/v1\",\n",
    "    api_key = 'EMPTY',\n",
    "    model_name = model_name,\n",
    "    temperature = 0.7\n",
    ")\n",
    "\n",
    "# ëª¨ë“  ëŒ€í™” ë‚´ìš©ì„ ëˆ„ì \n",
    "# Full-context\n",
    "history = [\n",
    "    SystemMessage(content = \"You are a helpful assistant\"),\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    history.append(HumanMessage(content = user_input))\n",
    "    \n",
    "    response = llm(history)\n",
    "    print(f\"Boat: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474d51d6",
   "metadata": {},
   "source": [
    "## NIM(NVIDIA Inference Microservice)\n",
    "\n",
    "NIMì€ ì—”í„°í”„ë¼ì´ì¦ˆ í™˜ê²½ì—ì„œ AI ëª¨ë¸ì„ ì‰½ê³  ì•ˆì •ì ìœ¼ë¡œ ë°°í¬í•˜ê¸° ìœ„í•œ ì—”ë¹„ë””ì•„ì˜ í†µí•© ì†”ë£¨ì…˜ìœ¼ë¡œ, AI ëª¨ë¸ì„ ì»¨í…Œì´ë„ˆí™”ëœ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ í˜•íƒœë¡œ ì œê³µí•˜ì—¬ ë°°í¬ë¥¼ ë‹¨ìˆœí™”í•˜ê³ , ìµœì í™”ëœ ì„±ëŠ¥ì„ ë³´ì¥í•œë‹¤. Localì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©° Local deviceì—ì„œì˜ ìµœì í™”ê°€ ìë™ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ìˆê³ , Privacyí•œ LLMì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤. \n",
    "\n",
    "### NIM íŠ¹ì§• \n",
    "- ì»¨í…Œì´ë„ˆ: AIëª¨ë¸ê³¼ í•„ìš”í•œ ëª¨ë“  êµ¬ì„± ìš”ì†Œë¥¼ ì»¨í…Œì´ë„ˆ í˜•íƒœë¡œ íŒ¨í‚¤ì§•í•˜ì—¬ í´ë¼ìš°ë“œ, ë°ì´í„° ì„¼í„°, ê·¸ë¦¬ê³  ë¡œì»¬ ë“± ì–´ë–¤ í™˜ê²½ì—ì„œë„ ì¼ê´€ë˜ê²Œ ëª¨ë¸ì„ í™œìš©í•  ìˆ˜ ìˆë‹¤.\n",
    "- ì„±ëŠ¥ ìµœì í™”: TensorRTë‚˜ vLLM ë“± ìµœì í™” ì—”ì§„ì´ ë‚´ì¥ë˜ì–´ìˆì–´ ëª¨ë¸ì˜ ì¶”ë¡  ì„±ëŠ¥ì„ ê·¹ëŒ€í™” ì‹œí‚¬ ìˆ˜ ìˆë‹¤.\n",
    "- í‘œì¤€ API: ê°œë°œìë“¤ì´ ê°„ë‹¨í•˜ê²Œ ëª¨ë¸ì„ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì„¤ê³„ í†µí•©í•  ìˆ˜ ìˆë‹¤. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f6a00",
   "metadata": {},
   "source": [
    "### NVIDIA Chat\n",
    "\n",
    "NIMì˜ ê¸°ëŠ¥ì„ ë­ì²´ì¸ê³¼ ì—°ë™í•˜ì—¬ ì‚¬ìš©í•œë‹¤. ë¡œì»¬ì—ì„œ ì‚¬ìš©í•˜ë”ë¼ë„ í´ë¼ìš°ë“œì—ì„œ ì—°ì‚°ì„ ì²˜ë¦¬í•˜ë¯€ë¡œ, Large Model ë“¤ë„ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤. \n",
    "\n",
    "nvidiaì—ì„œ ì§€ì›í•˜ëŠ” ëª¨ë¸ ì¤‘ `microsoft/phi-3.5-vision-instruct`ã…¡ `google-gemma3` ë“±ì˜ ëª¨ë¸ì„ Servingê°€ëŠ¥í•˜ë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adae982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA API Key Loaded: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.././.env\")\n",
    "\n",
    "nvidia_api_key = os.getenv('NVIDIA_API_KEY')\n",
    "print(f\"NVIDIA API Key Loaded: {nvidia_api_key is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80438725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ì €ëŠ” ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì´ë©°, Transformer ê¸°ë°˜ì˜ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ ê¸°ìš¸ì„ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•˜ê³  ë¬¸ì¥ì˜ ì˜ë¯¸ë¥¼ ì´í•´í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content = \"ë‹¹ì‹ ì€ ìœ ìš©í•œ ì¸ê³µì§€ëŠ¥ ë¹„ì„œì…ë‹ˆë‹¤. ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”\"),\n",
    "    HumanMessage(content = \"ì•ˆë…• gemmaëŠ” ì–´ë–¤ ëª¨ë¸ì´ì•¼?\")\n",
    "]\n",
    "\n",
    "llm = ChatNVIDIA(model = 'google/gemma-7b')\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b1415d",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "\n",
    "ë¡œì»¬ í™˜ê²½ì—ì„œ LLMì„ ì‰½ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ëœ ì‚¬ìš©ì ì¹œí™”ì  ë„êµ¬ì´ë‹¤. ë³µì¡í•œ ì„¤ì • ì—†ì´ë„ ëª‡ ê°€ì§€ ëª…ë ¬ì–´ë§Œìœ¼ë¡œ ë¡œì»¬ pcì—ì„œ ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì„ êµ¬ë™í•  ìˆ˜ ìˆë‹¤. \n",
    "\n",
    "- `ollama list` ëª…ë ¹ì–´ë¡œ pull ë˜ì–´ ìˆëŠ” LLM í™•ì¸ ê°€ëŠ¥ \n",
    "- `ollama run llama3`, `ollama pull gemma3-lateset` ë¡œ ì‹¤í–‰ ê°€ëŠ¥\n",
    "- í´ë¼ìš°ë“œë‚˜ ì™¸ë¶€ ì„œë²„ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , ê°œì¸ PCë‚˜ ë…¸íŠ¸ë¶ì—ì„œ LLMì„ êµ¬ë™í•˜ë¯€ë¡œ ë°ì´í„° í”„ë¼ì´ë²„ì‹œê°€ ì¤‘ìš”í•œ ê²½ìš°ì— í™œìš© ê°€ëŠ¥í•˜ë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f889f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì§€ì—­ì— ë”°ë¼ ë‹¤ë¦…ë‹ˆë‹¤. ì–´ë””ì— ê³„ì‹ ê°€ìš”? \\n\\n*   **ì„œìš¸:** ë§‘ê³  ê±´ì¡°í•˜ë©°, ë‚® ìµœê³ ê¸°ì˜¨ì€ 28ë„ê¹Œì§€ ì˜¬ë¼ê°‘ë‹ˆë‹¤.\\n*   **ë¶€ì‚°:** êµ¬ë¦„ì´ ì¡°ê¸ˆ ìˆìœ¼ë©°, ë‚® ìµœê³ ê¸°ì˜¨ì€ 26ë„ì…ë‹ˆë‹¤.\\n*   **ëŒ€êµ¬:** ë§‘ê³  ê±´ì¡°í•˜ë©°, ë‚® ìµœê³ ê¸°ì˜¨ì€ 30ë„ì…ë‹ˆë‹¤.\\n\\nì¢€ ë” ìì„¸í•œ ë‚ ì”¨ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´, ê³„ì‹  ì§€ì—­ì„ ì•Œë ¤ì£¼ì„¸ìš”. ğŸ˜Š' additional_kwargs={} response_metadata={'model': 'gemma3', 'created_at': '2025-08-03T06:30:14.0586027Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1950441700, 'load_duration': 82384300, 'prompt_eval_count': 19, 'prompt_eval_duration': 281028500, 'eval_count': 118, 'eval_duration': 1585276300, 'model_name': 'gemma3'} id='run--bbf3a08d-39d8-4f21-966c-59a614926559-0' usage_metadata={'input_tokens': 19, 'output_tokens': 118, 'total_tokens': 137}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm_ollama = ChatOllama(\n",
    "    model = 'gemma3',\n",
    "    base_url = 'http://localhost:11434',\n",
    "    temperature = 0.7\n",
    ")\n",
    "\n",
    "response_ollama = llm_ollama.invoke(\"ì•ˆë…•í•˜ì„¸ìš”? ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë–¤ê°€ìš”?\")\n",
    "\n",
    "print(response_ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2866ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì˜ ìµœì‹  ë°œì „ì„ ì´ëŒê³  ìˆëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ë” ì‘ì€ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©° NLP ê¸°ëŠ¥ì„ ê°–ì¶˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•˜ëŠ” ê°œë°œìë“¤ì—ê²Œ ì¤‘ìš”í•œ ë„êµ¬ì…ë‹ˆë‹¤.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "model = ChatOllama(\n",
    "    model = 'gemma3',\n",
    "    base_url = 'http://localhost:11434',\n",
    "    temperature = 0.7\n",
    ")\n",
    "\n",
    "# from_templateëŠ” ë¯¸ë¦¬ ì •ì˜ëœ ë¬¸ìì—´ í…œí”Œë¦¿ìœ¼ë¡œë¶€í„° í”„ë¡¬í”„íŠ¸ ê°ì²´ ìƒì„± \n",
    "template = PromptTemplate.from_template(\n",
    "    '''ì•„ë˜ì— ì‘ì„±í•œ ì»¨í…ìŠ¤íŠ¸(context)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸(question)ì— ëŒ€ë‹µí•˜ì„¸ìš”. ì œê³µëœ ì •ë³´ë¡œ ëŒ€ë‹µí•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì´ë¼ë©´ 'ëª¨ë¥´ê² ì–´ìš”'ë¼ê³  ë‹µí•˜ì„¸ìš”\n",
    "    \n",
    "Context : {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: '''\n",
    ")\n",
    "\n",
    "prompt = template.invoke({\n",
    "    \"context\":\"ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ìì—°ì–´ì²˜ë¦¬ ë¶„ì–‘ì˜ ìµœì‹  ë°œì „ì„ ì´ëŒê³  ìˆë‹¤. ë” ì‘ì€ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©° NLP ê¸°ëŠ¥ì„ ê°–ì¶˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•˜ëŠ” ê°œë°œìë“¤ì—ê²Œ ë§¤ìš° ì¤‘ìš”í•œ ë„êµ¬ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\",\n",
    "    'question':\"ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "})\n",
    "\n",
    "completion = model.invoke(prompt)\n",
    "completion.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
