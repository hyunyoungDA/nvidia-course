{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f08a0d6",
   "metadata": {},
   "source": [
    "<img src=\"https://www.e4ds.com/news_photo/U77C53G6CP8ASEHUJ5B7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7cbd6",
   "metadata": {},
   "source": [
    "# Encoder Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09d6f751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9879509806632996,\n",
       "  'token': 1029,\n",
       "  'token_str': '?',\n",
       "  'sequence': 'hello, mr. bert! how is it?'},\n",
       " {'score': 0.011153294704854488,\n",
       "  'token': 999,\n",
       "  'token_str': '!',\n",
       "  'sequence': 'hello, mr. bert! how is it!'},\n",
       " {'score': 0.0007006392115727067,\n",
       "  'token': 1012,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'hello, mr. bert! how is it.'},\n",
       " {'score': 0.00018348416779190302,\n",
       "  'token': 1025,\n",
       "  'token_str': ';',\n",
       "  'sequence': 'hello, mr. bert! how is it ;'},\n",
       " {'score': 5.2711493481183425e-06,\n",
       "  'token': 2133,\n",
       "  'token_str': '...',\n",
       "  'sequence': 'hello, mr. bert! how is it...'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import FillMaskPipeline, AutoModelForMaskedLM, AutoTokenizer, BertTokenizer \n",
    "\n",
    "model_ckpt = 'bert-base-uncased'\n",
    "\n",
    "unmasker = FillMaskPipeline(\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_ckpt),\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_ckpt)\n",
    ")\n",
    "\n",
    "unmasker('Hello, Mr. Bert! How is it [MASK]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feea19af",
   "metadata": {},
   "source": [
    "## 1. The Token Prediction Task Head\n",
    "\n",
    "BERT와 같은 Transformer Encoder 기반 모델은 기본적으로 `contextual embedding`을 생성하는 base model이다. 따라서 이 상태에서는 단순히 문장의 각 토큰에 대한 의미 정보만 담고 있고, 어떤 태스크도 직접 수행하지 않는다. \n",
    "\n",
    "즉, BERT base model에 downstream task에 맞는 head(classifier, regressor) 층을 추가로 올려야된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5195c8ff",
   "metadata": {},
   "source": [
    "### Masked Language Modeling \n",
    "- Pretraining에서 사용하고 일부 토큰을 마스킹하고 해당 위치의 토큰을 예측하는 방식\n",
    "- 각 토큰마다 Linear + softmax (vocabulary size 만큼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f94049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics From Input:\n",
      " > Input Indices: tensor([[ 101, 2360,  103,  999,  102]], device='cuda:0')\n",
      " > Input Decoding: ['[CLS]', 'say', '[MASK]', '!', '[SEP]']\n",
      " > Mask Index: 2\n",
      "\n",
      "Statistics From Forward Pass:\n",
      "> Input Into BERT Encoder: torch.Size([1, 5, 768])\n",
      "> Input Into Classifier:   torch.Size([1, 5, 768])\n",
      "> Output From Classifier:  torch.Size([1, 5, 30522])\n",
      "\n",
      "Statistics From BERT Output:\n",
      " > Most-Likely Index: tensor([1012, 2360, 2009,  999, 1012])\n",
      " > Most-Likely Probs: tensor([0.0358, 0.8322, 0.3312, 0.9999, 0.9998])\n",
      " > Most Likely Words: ['.', 'say', 'it', '!', '.']\n",
      "\n",
      "Statistics From Postprocessing (Top 5):\n",
      " > Most Likely Mask Index: tensor([2009, 2748, 2242, 2053, 7592], device='cuda:0')\n",
      " > Most Likely Mask Probs: tensor([0.3312, 0.1745, 0.1557, 0.0509, 0.0452], device='cuda:0')\n",
      " > Most Likely Mask Words: ['it', 'yes', 'something', 'no', 'hello'] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.33123713731765747,\n",
       " 'token': 2009,\n",
       " 'token_str': 'it',\n",
       " 'sequence': 'say it!'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, FillMaskPipeline, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n",
    "class MyFillMaskModel(FillMaskPipeline):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
    "            model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\").to('cuda')\n",
    "        )\n",
    "\n",
    "    def __call__(self, string):\n",
    "        \n",
    "        input_tensors = unmasker.preprocess(string)\n",
    "        input_tensors = input_tensors.to('cuda')\n",
    "        mask_idx = (input_tensors['input_ids'] == 103).nonzero()[0][1].item()\n",
    "        print(\"\\nStatistics From Input:\")\n",
    "        print(\" > Input Indices:\", input_tensors['input_ids'])\n",
    "        print(\" > Input Decoding:\", [unmasker.tokenizer.decode(token_index) for token_index in input_tensors['input_ids'][0]])\n",
    "        print(\" > Mask Index:\", mask_idx)\n",
    "\n",
    "        ## This is what we get throughout the model forward pass\n",
    "        inputs_1 = {'input_ids' : input_tensors['input_ids']}\n",
    "        inputs_2 = {'attention_mask' : input_tensors['attention_mask'].bool()}\n",
    "        embed_out = unmasker.model.bert.embeddings.forward(**inputs_1)\n",
    "        bert_out = unmasker.model.bert.encoder.forward(embed_out, **inputs_2)['last_hidden_state']\n",
    "        y = unmasker.model.cls.forward(bert_out)\n",
    "        print(\"\\nStatistics From Forward Pass:\")\n",
    "        print(\"> Input Into BERT Encoder:\", embed_out.shape)\n",
    "        print(\"> Input Into Classifier:  \", bert_out.shape)\n",
    "        print(\"> Output From Classifier: \", y.shape) # BERT의 vocab_size \n",
    "\n",
    "        ## The following statistics are generic outputs from the BERT differentiable pipeline\n",
    "        pdfs = torch.softmax(y[0], -1) \n",
    "        print(\"\\nStatistics From BERT Output:\")\n",
    "        print(\" > Most-Likely Index:\", torch.tensor([torch.argmax(pdf).item() for pdf in pdfs]))\n",
    "        print(\" > Most-Likely Probs:\", torch.tensor([torch.max(pdf).item() for pdf in pdfs]))\n",
    "        # max인 predicted token들을 decoding을 통해 확인 \n",
    "        print(\" > Most Likely Words:\", [unmasker.tokenizer.decode(torch.argmax(pdf).item()) for pdf in pdfs])\n",
    "\n",
    "        k = 5\n",
    "        mask_top_probs = torch.topk(pdfs[mask_idx], k) # topk -> 상위 k개 return \n",
    "        mask_best_words = [unmasker.tokenizer.decode(index) for index in mask_top_probs.indices]\n",
    "        print(f\"\\nStatistics From Postprocessing (Top {k}):\")\n",
    "        print(\" > Most Likely Mask Index:\", mask_top_probs.indices)\n",
    "        print(\" > Most Likely Mask Probs:\", mask_top_probs.values.detach())\n",
    "        print(\" > Most Likely Mask Words:\", mask_best_words, \"\\n\")\n",
    "\n",
    "        # Numpy로 처리하기 전에 cpu()로 device 변환 \n",
    "        output = self.postprocess({**input_tensors.to('cpu'), 'logits' : y})        \n",
    "        return output\n",
    "\n",
    "\n",
    "unmasker = MyFillMaskModel()\n",
    "unmasker(\"Say [MASK]!\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5fe057",
   "metadata": {},
   "source": [
    "## 2. SQuAD(Srandford Question Answering Dataset)\n",
    "\n",
    "SQuAD는 자연어 처리에서 대표적인 Extractive Question Answering(추출 기반 질문응답) 데이터셋이다.\n",
    "\n",
    "- 형태\n",
    "    - `context`: 하나의 문단\n",
    "    - `question`: 질문\n",
    "    - `answer`: 문단(context)에서 정확히 일치하는 **span**을 찾아내는 형식 \n",
    "\n",
    "- 출력: `context` 안에서 답변의 시작/끝 위치를 예측하여 해당 텍스트 span을 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6146d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\torchenv\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--deepset--roberta-base-squad2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cuda:0\n",
      "c:\\Users\\user\\anaconda3\\envs\\torchenv\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:390: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.21171413362026215,\n",
       " 'start': 59,\n",
       " 'end': 84,\n",
       " 'answer': 'gives freedom to the user'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "# Pre-trained RoBERTa model + QA head \n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'Why is model conversion important?',\n",
    "    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n",
    "}\n",
    "nlp(QA_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8350011d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForQuestionAnswering(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dfbd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.model.qa_outputs\n",
    "# 여기서 out_features는 start와 end의 의미인데, \n",
    "# 즉 in_features로 768를 받은 후에 question에 가장 적합한 부분의 시작 부분 start와 끝 부분 end를 return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aab18af",
   "metadata": {},
   "source": [
    "## 3. RoBERTa Sentiment Classifier code\n",
    "\n",
    "BERT나 다른 encoder 기반 모델 위에 분류기(classifier) head를 얹어서 downstream task를 해결하는 방식이다. SQuAD와는 다르게, **감정 분류(sentiment classification)** 은 전체 문장을 보고 증정, 부정, 중립 중 하나를 예측하는 task에 맞게 조정한다. \n",
    "\n",
    "RoBERTa classifier source code: https://github.com/huggingface/transformers/blob/f26099e7b5cf579f99a42bab6ddd371bf2c8d548/src/transformers/models/roberta/modeling_roberta.py#L1510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "902d18a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\torchenv\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'love', 'score': 0.9212924242019653}]\n",
      "[{'label': 'curiosity', 'score': 0.38752278685569763}]\n",
      "[{'label': 'confusion', 'score': 0.7724317312240601}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "emo_model = pipeline('sentiment-analysis', 'SamLowe/roberta-base-go_emotions')\n",
    "\n",
    "print(emo_model(\"I love my old pillow?\"))\n",
    "print(emo_model(\"Why is it that every plant I touch dies within a few days?\"))\n",
    "print(emo_model(\"I'm so conflicted about these new instructions...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d81e864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaClassificationHead(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (out_proj): Linear(in_features=768, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_model.model.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed8b798",
   "metadata": {},
   "source": [
    "## 4. Zero-shot Classification\n",
    "\n",
    "1. Zero-shot learning\n",
    "    - 학습 데이터에 해당 태스크나 레이블이 전혀 포함되지 않은 상태에서도 모델이 일반화된 지식을 활용해 문제를 푸는 방식 \n",
    "    - 예시: \"영화 리뷰를 긍정/부정으로 분류하세요\"라는 감정분석 태스크를 훈련한 적이 없는 모델에게 자연어로 설명만 주고 분류하게 하는 경우.\n",
    "\n",
    "2. Few-shot learning\n",
    "    - 새로운 태스크에 대해 **소량의 예시(샘플 몇 개)**만을 보고도 문제를 해결하는 학습 방식.\n",
    "    - 예시: 감정분석 예시를 2~5개 정도 제시한 후, 유사한 문장에 대해 감정을 예측하게 하는 경우.\n",
    "\n",
    "facebook/bart-large-mnli: https://huggingface.co/facebook/bart-large-mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b7eacda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\torchenv\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'one day I will see the world',\n",
       " 'labels': ['travel', 'dancing', 'cooking'],\n",
       " 'scores': [0.9938651323318481, 0.0032737762667238712, 0.00286104460246861]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "sequence_to_classify = \"one day I will see the world\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
