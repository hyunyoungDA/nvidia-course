{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5e139d3",
   "metadata": {},
   "source": [
    "<img src=\"https://www.e4ds.com/news_photo/U77C53G6CP8ASEHUJ5B7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f842b443",
   "metadata": {},
   "source": [
    "# 인코더, 디코더, 그리고 인코더-디코더 구조\n",
    "\n",
    "- 명시적 표현(Explicit representation): 사람이 읽거나 소프트웨어가 바로 이해할 수 있는 데이터 형태 (예: 문장, 이미지, 데이터 포인트 등)\n",
    "- 암묵적 표현(Implicit representation): 모델이 학습 과정에서 자동으로 만들어내는, 목표를 위해 최적화된 내부 표현 (예: 임베딩, 중간 레이어의 출력 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503acd8",
   "metadata": {},
   "source": [
    "## GPT(Generate Pre-train Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d73d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello world, all for a good ol' boys-who-come-up-in-town\"},\n",
       " {'generated_text': 'Hello world!\\n\\nThe game, named The Way of the Sun after God, is an ancient'},\n",
       " {'generated_text': \"Hello world, they always try to change it in different ways: for instance if they're not giving\"},\n",
       " {'generated_text': 'Hello world is full of surprises — every once-in-a-lifetime opportunity for the media'},\n",
       " {'generated_text': 'Hello world, you are my best friend.\\n\\nI am not your god\\n\\nA god'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed \n",
    "\n",
    "generator = pipeline('text-generation', model = 'gpt2')\n",
    "\n",
    "# max_length -> return되는 token의 개수 \n",
    "generator(\"Hello world\", max_length = 20, num_return_sequences = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eba8012",
   "metadata": {},
   "source": [
    "### 한 번에 모든 토큰 생성 \n",
    "\n",
    "- 병렬로 토큰을 생성하므로 속도 면에서 빠르다.\n",
    "- 중간 개입이 어려우며, 통계 기반 예측을 한 번에 적용 \n",
    "- `max_new_tokens = 20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0043ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING ALL AT ONCE:\n",
      "{'input_ids': tensor([[15496,   995]]), 'attention_mask': tensor([[1, 1]]), 'prompt_text': 'Hello world'}\n",
      "{'generated_sequence': tensor([[[15496,   995,     0,   770,   318,   534,  1110,  2474,   198,   198,\n",
      "              1,  5195,    11,   523,   703,   750,   428,  1645,  1701,   198,\n",
      "            198,    51]]]), 'input_ids': tensor([[15496,   995]]), 'prompt_text': 'Hello world'}\n",
      "[{'generated_text': 'Hello world! This is your day!\"\\n\\n\"Why, so how did this happen?\"\\n\\nT'}]\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "print(\"GENERATING ALL AT ONCE:\")\n",
    "input_str = 'Hello world'\n",
    "\n",
    "print(f\"{(x := generator.preprocess(input_str))}\") # encoding을 해서 model에 입력 \n",
    "print(f\"{(x := generator.forward(x, max_new_tokens=20))}\")\n",
    "print(f\"{(x := generator.postprocess(x))}\") # decoding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4152f6",
   "metadata": {},
   "source": [
    "### 한 번에 한 토큰씩 생성\n",
    "- 토큰마다 모델을 호출하므로 느림\n",
    "- 메모리 사용량이 많음 (매번 전체 context 재입력)\n",
    "- 매 토큰마다 제어 가능\n",
    "```python\n",
    "for i in range(20):\n",
    "    x = generator.forward(x, max_new_tokens = 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984b0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GENERATING ONE TOKEN AT A TIME (prep+forward+post):\n",
      "Hello world,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " we'll be going into detail by the next day. The rest of this episode is"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " an extension"
     ]
    }
   ],
   "source": [
    "print(\"\\nGENERATING ONE TOKEN AT A TIME (prep+forward+post):\")\n",
    "print(input_str := \"Hello world\", end=\"\")\n",
    "output_buffer = \"\"\n",
    "for i in range(20):\n",
    "    # 전치리와 forward, 그리고 decoding을 각 타임마다 한 번만 수행 \n",
    "    x = generator.preprocess(input_str + output_buffer)\n",
    "    x = generator.forward(x, max_new_tokens=1)\n",
    "    x = generator.postprocess(x)\n",
    "    # GPT-2는 decoder-only 모델이므로 매번 이전 토큰 전체를 다시 입력으로 받은 후 다음 토큰 생성\n",
    "    # input_str + output_buffer 만큼 x에서 get해서 다음 토큰 생성에 사용 \n",
    "    next_word = x[0].get(\"generated_text\")[len(input_str + output_buffer):]\n",
    "    output_buffer += next_word\n",
    "    print(next_word.replace(\"\\n\", \"\\\\n\"), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf739fa",
   "metadata": {},
   "source": [
    "### GPT-2 모델을 사용하여 토큰을 하나씩 수동으로 생성하는 로우레벨 구현\n",
    "\n",
    "- 저수준 로직을 직접 구현하여 세밀하게 제어 가능\n",
    "- GPT-2 `transformer` 직접 호출 (앞서 얘기한 방법들은 Hugging face가 처리)\n",
    "- 제어력 강함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e795876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "GENERATING ONE TOKEN AT A TIME (manually, greedily-sampled):\n",
      "<|endoftext|> Hello world"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nGENERATING ONE TOKEN AT A TIME (manually, greedily-sampled):\")\n",
    "\n",
    "# Transformer의 model_body, head, tokenizer.encode, tokenizer.decode를 직접 호출해서 사용 \n",
    "model_body = generator.model.transformer.to('cuda') # transformer의 GPT-2 모델을 호출해서 사용 \n",
    "model_head = generator.model.lm_head.to('cuda')\n",
    "tknzr_encode = generator.tokenizer.encode\n",
    "tknzr_decode = generator.tokenizer.decode\n",
    "\n",
    "def compute_embed(token_id):\n",
    "    device = model_body.wte.weight.device\n",
    "    # GPT-2의 word toekn embedding layer(wte)로 token_id를 임베딩 벡터로 변환 \n",
    "    return model_body.wte(torch.tensor([token_id], device = device)).view(1, -1, 768)\n",
    "\n",
    "# PREFILL stage: Processing the initial input string\n",
    "print(input_str := \"<|endoftext|> Hello world\", end=\"\")\n",
    "# inputs = {k: v.to('cuda') for k, v in }\n",
    "embed_buffer = compute_embed(tknzr_encode(input_str))\n",
    "attention_mask = torch.ones(embed_buffer.shape[:2], dtype=torch.long) # 1로 채우면서 모든 토큰이 유효함 표시 \n",
    "past_key_values = None # past_key_value 초기화 \n",
    "\n",
    "# PREFILL - running the model for the input string, getting kv cache and embeddings\n",
    "prefill_output = model_body.forward(\n",
    "    inputs_embeds=embed_buffer, # token_id 대신 미리 정의한 임베딩 벡터를 직접 넘김 \n",
    "    attention_mask=attention_mask, # attention mask \n",
    "    past_key_values=past_key_values,\n",
    ")\n",
    "past_key_values = prefill_output.get(\"past_key_values\") # 각 레이어의 K/V cache이며 다음 토큰 계산 시 재사용 \n",
    "predicted_embed = prefill_output.get(\"last_hidden_state\") # 각 토큰의 최종 출력 벡터 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b72207ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "\n",
      "I'm a programmer and I'm a big fan of the Java programming language. I've been using Java since I was a kid and I've been using it for a long time. I've been using Java for a long time and I've been using it for a long time. I've been using Java for a long time and I've been using it for a long time. I've been using Java for a long time and I've been using it for a long time."
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    predicted_probs = model_head(predicted_embed[:, -1, :]) # 가장 최근 토큰의 임베딩만 추출 \n",
    "    predicted_token = torch.argmax(predicted_probs, dim=-1).item()\n",
    "    print(tknzr_decode(predicted_token), end=\"\") # 다음에 올 확률이 가장 큰 token을 decoding하여 확인 \n",
    "\n",
    "    # Update attention mask and run model with past_key_values for next token\n",
    "    decode_output = model_body.forward(\n",
    "        inputs_embeds=compute_embed(predicted_token), \n",
    "        attention_mask=torch.ones([1,1], dtype=torch.long),\n",
    "        past_key_values=past_key_values,\n",
    "    )\n",
    "    predicted_embed = decode_output.get(\"last_hidden_state\")\n",
    "    past_key_values = decode_output.get(\"past_key_values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fafdf6",
   "metadata": {},
   "source": [
    "## T5(Text-to-Text Transfer Transformer)\n",
    "\n",
    "T5는 Google에서 2019년에 발표한 자연어처리 모델로, 모든 NLP 작업을 **Text-to-Text** 문제로 통일해서 처리하는 프레임워크이다. 기존에는 분류, 번역 ,요약, 질의응답 등 NLP task마다 서로 다른 아키텍처나 출력 형식을 사용하였는데 이를 하나의 통합된 모델이 다양한 작업을 처리할 수 있게 만들었다.\n",
    "\n",
    "- 두 개 이상의 시퀀스를 다루고, 이 시퀀스들의 길이가 다를 수 있을 때\n",
    "    - 만약 입력과 출력 길이가 같거나 출력이 입력의 부분집합이라면 인코더만으로 충분하다.\n",
    "- 출력 시퀀스를 점진적으로 생성해야 할 때\n",
    "    - 출력이 고정되어 있거나 요약하는 수준이라면 인코더만으로 충분하다.\n",
    "- 입력과 출력 시퀀스가 서로 다른 분포(형식, 목적, 데이터 유형 등)를 따를 때\n",
    "    - 동일한 분포라면 같은 네트워크 경로를 사용하는 게 더 낫다.\n",
    "- 경량화나 특정 작업에 특화된 모델이 필요할 때\n",
    "    - 범용 모델이라면 디코더 경로를 활용하는 편이 나을 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1ec53b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\torchenv\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Bonjour Monde, comment se passe-t-il ?'},\n",
       " {'translation_text': 'Quel est votre nom?'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline('translation_en_to_fr', model = 't5-base', device = 'cuda')\n",
    "translator([\"Hello World! How's it going?\", \"What's your name?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ac0fb4",
   "metadata": {},
   "source": [
    "### Preprocessing & Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16a54e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs Into Preprocessing: Hello | World | ! | How | ' | s | it | going | ? | </s>\n",
      "Inputs Into Model Forward: translate | English | to | French | : | Hello | World | ! | How | ' | s | it | going | ? | </s>\n",
      "Output From Model Forward: Bonjour | Monde | , | comment | se | passe | - | t | - | il |  | ? | </s>\n"
     ]
    }
   ],
   "source": [
    "text_en = \"Hello World! How's it going?\"\n",
    "resp_fr = translator(text_en)\n",
    "text_fr = resp_fr[0]['translation_text']\n",
    "\n",
    "tknzr = translator.tokenizer\n",
    "tokens_ins = [tknzr.decode(x) for x in tknzr.encode(text_en)]\n",
    "tokens_in2 = [tknzr.decode(x) for x in translator.preprocess(text_en)['input_ids'][0]]\n",
    "tokens_out = [tknzr.decode(x) for x in tknzr.encode(text_fr)]\n",
    "print(f\"Inputs Into Preprocessing: {' | '.join(tokens_ins)}\")\n",
    "print(f\"Inputs Into Model Forward: {' | '.join(tokens_in2)}\")\n",
    "print(f\"Output From Model Forward: {' | '.join(tokens_out)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bbc13a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Bonjour| Monde|,| comment| se| passe|-|t|-|il| |?|"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_token_generator(pipeline, model=None, tokenizer=None, max_tokens=50):\n",
    "    \n",
    "    model = pipeline.model or model\n",
    "    tknzr = pipeline.tokenizer or tokenizer\n",
    "    # getattr: model 객체에 encoder라는 속성이 있으면 그 값을 반환하고, 없으면 None을 반환 \n",
    "    encoder = getattr(model, \"encoder\", None)\n",
    "    decoder = ( ## Non-exhaustive resolution\n",
    "        getattr(model, \"decoder\", None) \n",
    "        or getattr(model, \"model\", None) \n",
    "        or getattr(model, \"transformer\", None)\n",
    "    )\n",
    "    lm_head = getattr(model, \"lm_head\", None)\n",
    "    dev = decoder.device\n",
    "    \n",
    "    def token_generator(\n",
    "        encoder_input: str = \"\",\n",
    "        decoder_input: str = \"\",\n",
    "        max_tokens: int = max_tokens\n",
    "    ):  \n",
    "        # encode(encoder_input)[:-1]은 eos_token을 제거하는 용도\n",
    "        # boolean을 활용하여 encoder_input이 빈 문자열이면 [] * 0 -> 오류 처리\n",
    "        encoder_input_idxs = tknzr.encode(encoder_input)[:-1] * bool(encoder_input)\n",
    "        decoder_input_idxs = tknzr.encode(decoder_input)[:-1] * bool(decoder_input)\n",
    "        decoder_inputs = {}\n",
    "\n",
    "        # 만약 encoder가 존재하면 input_ids 키를 가진 딕셔너리 생성 \n",
    "        if encoder:\n",
    "            encoder_inputs = {\"input_ids\": torch.tensor([encoder_input_idxs], device=dev)}\n",
    "            encoder_outputs = encoder(**encoder_inputs)\n",
    "            # encoider의 출력 임베딩을 디코더에 넘겨줘야 디코더가 인코더의 정보를 참고해서 출력을 생성할 수 있음 \n",
    "            decoder_inputs[\"encoder_hidden_states\"] = encoder_outputs.last_hidden_state\n",
    "        elif encoder_input_idxs:\n",
    "            print(\"`encoder_input` specified despite no encoder being available. Ignoring\")\n",
    "            \n",
    "        # pad_token_id가 존재하면 넣어주고 없다면 빈 리스트 return \n",
    "        buffer_token_idxs = [] if (tknzr.pad_token_id is None) else [tknzr.pad_token_id]\n",
    "        buffer_token_idxs += decoder_input_idxs # encoder의 출력 상태 더해줌 \n",
    "        buffer_token_str = \"\"\n",
    "        max_length = len(buffer_token_idxs) + max_tokens # max_length는 max_tokens 만큼 추가해서 정의 \n",
    "        while len(buffer_token_idxs) < max_length:\n",
    "            \n",
    "            decoder_inputs[\"input_ids\"] = torch.tensor([buffer_token_idxs], device=dev).long()\n",
    "            decoder_outputs = decoder(**decoder_inputs) # decoder는 지금까지 생성된 context를 바탕으로 다음 토큰 분포 예측\n",
    "            model_outputs = lm_head(decoder_outputs.last_hidden_state) # 토큰별 logits값 연산 \n",
    "        \n",
    "            ## Get the most likely next token and add it to the buffer\n",
    "            try: \n",
    "                sampled_token_idx = torch.argmax(model_outputs, -1)[0][-1].item()\n",
    "            except: \n",
    "                break\n",
    "            \n",
    "            buffer_token_idxs += [sampled_token_idx] # 새로 예측한 token ID를 생성 버퍼에 추가 \n",
    "            buffer_token_old = buffer_token_str # 현재까지 생성된 텍스트 \n",
    "            buffer_token_str = tknzr.decode(buffer_token_idxs)\n",
    "            buffer_token_new = buffer_token_str[len(buffer_token_old):] # 이번에 생성된 단어를 확인하기 위해 \n",
    "\n",
    "            ## Yield (output while keeping spot in the generator call) next token.\n",
    "            ## If it's end-of-string </s>, break the loop.\n",
    "            if sampled_token_idx == tknzr.eos_token_id:\n",
    "                break\n",
    "            if buffer_token_new:\n",
    "                yield buffer_token_new\n",
    "    \n",
    "    return token_generator\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "streamer = get_token_generator(translator)\n",
    "input_raw_str = \"translate English to French: Hello World! How's it going?</s>\"\n",
    "\n",
    "for token in streamer(encoder_input = input_raw_str):\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e61e82c",
   "metadata": {},
   "source": [
    "### Google의 `flan-t5-large` 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e351d3",
   "metadata": {},
   "source": [
    "<img src=\"https://velog.velcdn.com/images/choonsik_mom/post/275d137e-3cf4-42e2-841d-e19ba26908ee/image.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53456a8e",
   "metadata": {},
   "source": [
    "<img src=\"https://blog.kakaocdn.net/dna/8lWFY/btsngIa3NBA/AAAAAAAAAAAAAAAAAAAAAHSgJ1PhqhY8QqG2Tyw9HqvCob1YB6YuJF3vc4FvvkPU/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=aQn90Gr909Y4AmplK8KYpkmUUWE%3D\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690141dc",
   "metadata": {},
   "source": [
    "### Flan-T5 (Unsupervised learning)\n",
    "\n",
    "Flan version의 T5 모델은 더욱 복잡한 작업들을 통해 추가로 학습되어, 기존에 학습한 목적을 넘어서 `in-context leraning` 능력을 갖출 수 있게 한다. 즉, 이 모델은 새로운 작업에 대해 사전 훈련 없이도 단지 무엇을 해야 하는지 문맥 안에서 지시만 받으면 그 작업을 해결할 수 있는 능력을 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9283718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Bonjour, monde! Comment se passe-t-il?"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "flan_t5_pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
    "\n",
    "streamer = get_token_generator(flan_t5_pipe)\n",
    "input_raw_str = \"translate English to French: Hello World! How's it going?</s>\"\n",
    "\n",
    "for token in streamer(encoder_input = input_raw_str):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fe5e6b",
   "metadata": {},
   "source": [
    "### Encoder In-Context Learning \n",
    "\n",
    "Encoder 방식은 context를 먼저 다 이해한 후 text를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c7b8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate from English to Spanish: The book is on the table.\n",
      "<pad> El libro está en la mesa.. Example: El gato está dormido en el sofá.? El gato está dormido en \n",
      "\n",
      "Answer the question using commonsense knowledge: Why can't a fish live out of water?\n",
      "<pad> a fish needs water to breathe\n",
      "\n",
      "Continue the story with a creative twist: Once upon a time, in a forest far away, there was a small bear named Timmy who loved honey.\n",
      "<pad> The princess was very happy to see her castle floating in the sky.\n",
      "\n",
      "Solve the mathematical problem: What is the square root of 144?\n",
      "<pad> -12\n",
      "\n",
      "Answer based on factual knowledge: Who was the first person to walk on the moon?\n",
      "<pad> Neil Armstrong\n",
      "\n",
      "Continue the conversation naturally: User: Can you help me with directions? Agent:\n",
      "<pad> User: Great. Is there anything else I can do for you?\n",
      "\n",
      "Continue the conversation naturally: User: Can you help me with directions? Agent: Sure, where to and where from? User: I'd like to get from LA to San Jose today. What's the best road? Agent: \n",
      "<pad> User: That sounds good.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = [\n",
    "    {   # Simple Translation\n",
    "        \"premise\": \"Translate from English to Spanish\",\n",
    "        \"question\": \"The book is on the table.\",\n",
    "        \"answer\": \"El libro está en la mesa.\",\n",
    "        \"few_shot\": {\n",
    "            \"question\": \"The cat is sleeping on the sofa.\",\n",
    "            \"answer\": \"El gato está durmiendo en el sofá.\"\n",
    "    }},{# Commonsense Reasoning\n",
    "        \"premise\": \"Answer the question using commonsense knowledge\",\n",
    "        \"question\": \"Why can't a fish live out of water?\",\n",
    "        \"few_shot\": {\n",
    "            \"question\": \"Why can't humans breathe underwater?\",\n",
    "            \"answer\": \"Humans can't breathe underwater because they need air, not water, to fill their lungs.\"\n",
    "    }},{# Creative Story Generation\n",
    "        \"premise\": \"Continue the story with a creative twist\",\n",
    "        \"question\": \"Once upon a time, in a forest far away, there was a small bear named Timmy who loved honey.\",\n",
    "        \"few_shot\": {\n",
    "            \"question\": \"A princess woke up one day to find her castle floating in the sky.\",\n",
    "            \"answer\": \"As she looked outside, she saw a giant eagle carrying the castle on its back, flying towards the mountains.\"\n",
    "    }},{  # Mathematical Problem Solving\n",
    "        \"premise\": \"Solve the mathematical problem\",\n",
    "        \"question\": \"What is the square root of 144?\",\n",
    "        \"few_shot\": {\n",
    "            \"question\": \"What is the cube of 3?\",\n",
    "            \"answer\": \"27.\"\n",
    "    }},{# Fact-based Question Answering\n",
    "        \"premise\": \"Answer based on factual knowledge\",\n",
    "        \"question\": \"Who was the first person to walk on the moon?\",\n",
    "        \"few_shot\": {\n",
    "            \"question\": \"Who was the first president of the United States?\",\n",
    "            \"answer\": \"The first president of the United States was George Washington.\"\n",
    "    }},{# Conversational Continuation\n",
    "        \"premise\": \"Continue the conversation naturally\",\n",
    "        \"question\": \"User: Can you help me with directions? Agent:\",\n",
    "        \"few_shot\": {\n",
    "            \"question\": \"User: What’s the weather like today? Agent:\",\n",
    "            \"answer\": \"It’s sunny and warm with a light breeze.\"\n",
    "    }},{# Conversational Continuation\n",
    "        \"premise\": \"Continue the conversation naturally\",\n",
    "        \"question\": (\n",
    "            \"User: Can you help me with directions? Agent: Sure, where to and where from?\"\n",
    "            \" User: I'd like to get from LA to San Jose today. What's the best road? Agent: \"\n",
    "        ),\n",
    "        \"few_shot\": {\n",
    "            \"question\": \"User: What’s the weather like today? Agent:\",\n",
    "            \"answer\": \"It’s sunny and warm with a light breeze.\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "streamer = get_token_generator(flan_t5_pipe)\n",
    "\n",
    "# encoder 모델을 통해 few-shot\n",
    "# In-context Learning을 통해 추가 fine-tuning 없이도 문제 유형을 파악해서 답 생성 가능 \n",
    "for entry in dataset:\n",
    "    P, Q = entry['premise'], entry['question'] # prefix와 question을 P, Q에 각각 저장 \n",
    "    FSQ, FSA = entry['few_shot']['question'], entry['few_shot']['answer'] # few_shot의 question과 answer 각각 저장 \n",
    "    inputs = {\n",
    "        \"encoder_input\": f\"{P}: {Q}. Example: {FSQ}? {FSA}</s>\",\n",
    "        \"decoder_input\": \"\",\n",
    "        # \"encoder_input\": f\"{P}: \",\n",
    "        # \"decoder_input\": f\"{FSQ}? {FSA}</s>{Q}? \",\n",
    "    }\n",
    "    print(f\"{P}: {Q}\")\n",
    "    for token in streamer(**inputs):\n",
    "        print(token, end=\"\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa55f13",
   "metadata": {},
   "source": [
    "### Flan-T5 Decoder In-Context Learning\n",
    "\n",
    "GPT나 Ph1 등의 모델은 encoder가 없으며, 한 번에 모든 입력을 받고 그 뒤에 이어지는 다음 토큰을 하니씩 예측한다. \n",
    "context는 매 step마다 함께 들어가서 지속적으로 참고한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc370f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************\n",
      "Translate from English to Spanish: The cat is sleeping on the sofa.\n",
      "\n",
      "Answer: El gato está durmiendo en el sofá.\n",
      "\n",
      " Translate from English to Spanish: The book is on the table.\n",
      "\n",
      "Answer: La libro está en el table.\n",
      "\n",
      "Exercise 2:\n",
      "\n",
      "Fill in the blank with the correct word:\n",
      "\n",
      "The _ is a type of bird that can fly long distances.\n",
      "\n",
      "Answer: migratory\n",
      "\n",
      "Ex\n",
      "\n",
      "****************************************************************\n",
      "Answer the question using commonsense knowledge: Why can't humans breathe underwater?\n",
      "\n",
      "Answer: Humans can't breathe underwater because they need air, not water, to fill their lungs.\n",
      "\n",
      " Answer the question using commonsense knowledge: Why can't a fish live out of water?\n",
      "\n",
      "Answer: A fish can't live out of water because it needs water to breathe and survive.\n",
      "\n",
      "\n",
      "****************************************************************\n",
      "Continue the story with a creative twist: A princess woke up one day to find her castle floating in the sky.\n",
      "\n",
      "Answer: As she looked outside, she saw a giant eagle carrying the castle on its back, flying towards the mountains.\n",
      "\n",
      " Continue the story with a creative twist: Once upon a time, in a forest far away, there was a small bear named Timmy who loved honey.\n",
      "\n",
      "Answer: One day, Timmy decided to climb the tallest tree in the forest to get some honey. But as he was climbing, he realized that the tree was too tall for him to reach the honey.\n",
      "\n",
      "Exercise 2: Rewrite the following\n",
      "\n",
      "****************************************************************\n",
      "Solve the mathematical problem: What is the cube of 3?\n",
      "\n",
      "Answer: 27.\n",
      "\n",
      " Solve the mathematical problem: What is the square root of 144?\n",
      "\n",
      "Answer: 12.\n",
      "\n",
      " Solve the mathematical problem: What is the value of x in the equation 2x + 5 = 11?\n",
      "\n",
      "Answer: 3.\n",
      "\n",
      "\n",
      "****************************************************************\n",
      "Answer based on factual knowledge: Who was the first president of the United States?\n",
      "\n",
      "Answer: The first president of the United States was George Washington.\n",
      "\n",
      " Answer based on factual knowledge: Who was the first person to walk on the moon?\n",
      "\n",
      "Answer: The first person to walk on the moon was Neil Armstrong.\n",
      "\n",
      "Exercise 2:\n",
      "\n",
      "Read the following statement and determine if it is a fact or an opinion:\n",
      "\n",
      "\"Chocolate ice cream is the best flavor\n",
      "\n",
      "****************************************************************\n",
      "Continue the conversation naturally: User: What’s the weather like today? Agent:\n",
      "\n",
      "Answer: It’s sunny and warm with a light breeze.\n",
      "\n",
      " Continue the conversation naturally: User: Can you help me with directions? Agent:\n",
      "\n",
      "Answer: Sure, I can help you with directions.\n",
      "\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoder_pipe = pipeline(\"text-generation\", model=\"microsoft/phi-1_5\", stop_token=\"\\n\", device=\"cuda\")\n",
    "# decoder_pipe = pipeline(\"text-generation\", model=\"gpt2\", stop_token=\"\\n\")\n",
    "streamer = get_token_generator(decoder_pipe)\n",
    "\n",
    "for entry in dataset:\n",
    "    P, Q = entry['premise'], entry['question']\n",
    "    FSP, FSQ = entry['few_shot']['question'], entry['few_shot']['answer']\n",
    "    inputs = {\n",
    "        \"decoder_input\": f\"{P}: {FSP}\\n\\nAnswer: {FSQ}\\n\\n {P}: {Q}\\n\\nAnswer: \",\n",
    "    }\n",
    "    print(\"*\" * 64)\n",
    "    for token in streamer(**inputs):\n",
    "        print(token, end=\"\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "## EXERCISE: Incorporate Few-Shot (in this case just one-shot) conditioning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
