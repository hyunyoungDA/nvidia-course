{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3380d9b8",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FcK7IvP%2FbtrY7MoNroA%2FAAAAAAAAAAAAAAAAAAAAANRlYs3-AmJlv2XO44yQLk4xgPYAAnEOG6O2_zT5j1oP%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1753973999%26allow_ip%3D%26allow_referer%3D%26signature%3DvIusSJvEQZ7Ds%252Bi13VnJMEZLoHQ%253D' width = 300 >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349f96e4",
   "metadata": {},
   "source": [
    "## BERT(Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "Google의 자연어 처리 사전학습 모델(PLM)로 문맥을 **양방향**으로 이해할 수 있도록 설계된 모델이다. \n",
    "- `BERT-base` 기준 총 12개의 transformer encoder layer로 구성되어있다.\n",
    "    - Hidden size = 768\n",
    "    - Attention heads: 12\n",
    "    - Parameters: 약 110M(Million)\n",
    "\n",
    "### Pre-trained 목표\n",
    "\n",
    "1. **Masked Language Model(MLM)**: 입력 문장에서 전체 토큰 중 15%를 무작위로 마스킹하고, 이를 맞추는 언어모델링이다. 80%를 [MASK]로 대체한 후 10%는 랜덤한 다른 토큰으로, 나머지 10%는 원래 토큰 그대로 유지하여 계산한다. \n",
    "\n",
    "2. **Next Sentence Prediction (NSP)**: 두 개의 문장을 입력으로 받아, 두 번째 문장이 실제로 첫 번째 문장 뒤에 오는지 아닌지를 맞추는 이진 분류 문제이다. 전체 입력의 의미를 담는 [CLS] 토큰의 출력을 활용하여 NSP를 판단한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8b6d03",
   "metadata": {},
   "source": [
    "### Embedding layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fb8b09cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import math \n",
    "\n",
    "class BertTokenEmbedding(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size: int, \n",
    "                 embedding_dim: int = 768, \n",
    "                 max_position_embeddings: int = 512,\n",
    "                 type_vocab_size: int = 2, # 2개의 문장을 받으므로 \n",
    "                 dropout_p: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token = nn.Embedding(vocab_size, embedding_dim) # 입력 받은 sentence 임베딩 \n",
    "        self.segment = nn.Embedding(type_vocab_size, embedding_dim) \n",
    "        self.position = nn.Embedding(max_position_embeddings, embedding_dim)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    def forward(self, token_ids, segment_ids):\n",
    "        x = self.token(token_ids) + self.segment(segment_ids) + self.position(token_ids)\n",
    "        x = self.layer_norm(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4eace4",
   "metadata": {},
   "source": [
    "### Multi-Head Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "90721656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt \n",
    "\n",
    "def scaled_dot_product_attention(querys: torch.Tensor, \n",
    "                                 keys: torch.Tensor, \n",
    "                                 values: torch.Tensor, \n",
    "                                 attention_mask: torch.Tensor = None, # 패딩 마스크 \n",
    "                                 is_casual: bool = False):\n",
    "    dim_k = querys.size(-1)\n",
    "    scores = querys @ keys.transpose(-2, -1) / sqrt(dim_k)\n",
    "    \n",
    "    # 패딩 마스크가 0인 위치는 -inf로 설정하여 softmax 계산 시 0이 되도록 설정 \n",
    "    if attention_mask is not None:\n",
    "        scores = scores.masked_fill(attention_mask == 0, float('-inf'))\n",
    "\n",
    "    attention_weights = F.softmax(scores, dim = 1)\n",
    "    output = attention_weights @ values\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "40ddf71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"embedding_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        # 수정 \n",
    "        # self.seq_len = seq_len # size에서 추출 \n",
    "        self.dim_k = d_model // num_heads # 임베딩 차원을 head의 개수로 나눠서 각 head의 차원 계산\n",
    "        \n",
    "        self.weight_q = nn.Linear(d_model, d_model)\n",
    "        self.weight_k = nn.Linear(d_model, d_model)\n",
    "        self.weight_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # 멀티헤드이므로, 최종적으로 선형 변환 레이어 필요 \n",
    "        self.concat_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self,\n",
    "                query: torch.Tensor, \n",
    "                key: torch.Tensor, \n",
    "                value: torch.Tensor, \n",
    "                attention_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        # batch_size 값 추출 \n",
    "        batch_size = query.size(0)\n",
    "        seq_len = query.size(1)\n",
    "        # print(query.size()) Debug \n",
    "        \n",
    "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, num_heads, dim_k)\n",
    "        # seq_len으로 정의하지 않고 -1로 자동 정의할 때 오류 발생 가능성 O \n",
    "        query = self.weight_q(query).view(batch_size, seq_len, self.num_heads, self.dim_k).transpose(1,2)\n",
    "        key = self.weight_k(key).view(batch_size, seq_len, self.num_heads, self.dim_k).transpose(1, 2)\n",
    "        value = self.weight_v(value).view(batch_size, seq_len, self.num_heads, self.dim_k).transpose(1, 2)\n",
    "        \n",
    "        # (batch_size, num_heads, seq_len, dim_k) -> (batch_size, seq_len, d_model)\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(query, key, value, attention_mask)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        output = self.concat_linear(attn_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48585e1",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forwardr Network(FFN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "78e4cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, dim_feedforward: int, dropout_p: float):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        \n",
    "        self.activation = nn.GELU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        norm_x = self.norm(x)\n",
    "        \n",
    "        ffn_output = self.linear1(norm_x)\n",
    "        ffn_output = self.activation(ffn_output)\n",
    "        ffn_output = self.dropout(ffn_output)\n",
    "        \n",
    "        ffn_output = self.linear2(ffn_output)\n",
    "        \n",
    "        ffn_output = self.dropout(ffn_output)\n",
    "        output = x + ffn_output # 잔차 연결 residual connection\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862f552e",
   "metadata": {},
   "source": [
    "### Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "baed7b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dim_feedforward: int, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout_p)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, dim_feedforward, dropout_p)\n",
    "        \n",
    "    def forward(self, src, attention_mask = None) -> torch.Tensor:\n",
    "        norm_x = self.norm(src)\n",
    "        attn_output = self.attn(norm_x, norm_x, norm_x, attention_mask)\n",
    "        # 잔차 연결 \n",
    "        x = src + self.dropout1(attn_output)\n",
    "        x = self.feed_forward(x) \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14838fa",
   "metadata": {},
   "source": [
    "### BERT Encoder (N Layer Transformer Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8c7f993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_layers: int, \n",
    "                 d_model: int, \n",
    "                 num_heads: int, \n",
    "                 dim_feedforward: int, \n",
    "                 dropout_p: float):\n",
    "        super().__init__()\n",
    "        \n",
    "        # num_layers 개의 Transformer Encoder layer 생성 \n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerEncoder(d_model, num_heads, dim_feedforward, dropout_p) for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, attention_mask = None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259143a0",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fb60dfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 10\n",
    "d_model = 768\n",
    "num_heads = 12\n",
    "d_ff = 3072\n",
    "num_layers = 2\n",
    "dropout_p = 0.2\n",
    "\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "attention_mask = torch.ones(batch_size, 1, 1, seq_len)  # 모두 attend 가능\n",
    "\n",
    "encoder = BERTEncoder(num_layers, d_model, num_heads, d_ff, dropout_p)\n",
    "out = encoder(x, attention_mask)\n",
    "print(out.shape)  # (2, 10, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b775d93",
   "metadata": {},
   "source": [
    "### Classifier Layer 추가\n",
    "\n",
    "BERT는 원래 Encoder 모델로, MLM과 NSP고 사전 학습 되어있는 모델이다.따라서 대부분의 downstream task에서는 기존 BERT 구조에 **task-specific head**를 추가하여 해당 테스크를 해결한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "75bdd0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, encoder, d_model, num_classes, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.encoder = encoder\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        x = self.embedding(x)                 # (B, L) -> (B, L, D)\n",
    "        x = self.encoder(x, attention_mask)   # (B, L, D)\n",
    "        x = x.transpose(1, 2)                 # (B, D, L)\n",
    "        x = self.pool(x).squeeze(-1)          # (B, D)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "432f18e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 임의의 데이터셋 정의 \n",
    "class FakeDataset(Dataset):\n",
    "    def __init__(self, num_samples=200, seq_len=16, vocab_size=100, num_classes=2):\n",
    "        self.inputs = torch.randint(0, vocab_size, (num_samples, seq_len))\n",
    "        self.labels = torch.randint(0, num_classes, (num_samples,))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3903caf6",
   "metadata": {},
   "source": [
    "### `train()` 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f3c2be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total, correct = 0, 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    acc = correct / total * 100\n",
    "    print(f\"Train Accuracy: {acc:.2f}%\")\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cff9b517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 48.67%\n",
      "Train Accuracy: 58.67%\n",
      "Train Accuracy: 65.33%\n",
      "Train Accuracy: 66.33%\n",
      "Train Accuracy: 65.67%\n",
      "Train Accuracy: 69.00%\n",
      "Train Accuracy: 69.67%\n",
      "Train Accuracy: 68.33%\n",
      "Train Accuracy: 74.00%\n",
      "Train Accuracy: 75.33%\n",
      "Train Accuracy: 75.67%\n",
      "Train Accuracy: 80.00%\n",
      "Train Accuracy: 82.33%\n",
      "Train Accuracy: 82.67%\n",
      "Train Accuracy: 80.33%\n",
      "Train Accuracy: 87.00%\n",
      "Train Accuracy: 89.33%\n",
      "Train Accuracy: 93.67%\n",
      "Train Accuracy: 92.00%\n",
      "Train Accuracy: 96.33%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# BERT 인코더 설정 \n",
    "encoder = BERTEncoder(\n",
    "    num_layers=2,\n",
    "    d_model=32,\n",
    "    num_heads=4,\n",
    "    dim_feedforward=128,\n",
    "    dropout_p=0.1\n",
    ").to(device)\n",
    "\n",
    "model = BertClassifier(encoder, d_model=32, num_classes=2, vocab_size=100).to(device)\n",
    "\n",
    "dataset = FakeDataset(num_samples=300, seq_len=16, vocab_size=100, num_classes=2)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 학습 반복\n",
    "for epoch in range(20):\n",
    "    acc = train(model, loader, optimizer, criterion, device)\n",
    "    if acc == 100.0:\n",
    "        print(f\"Accuracy at Epoch {epoch + 1}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc544b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
